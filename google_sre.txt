Why can't we expect to build 100% reliable services? (2 reasons)	"Past a certain point, increasing reliability is worse for the service (and it's users) because it comes at a cost: speed of development and cost.<br><br>Further, users typically don’t notice the difference between high reliability and extreme reliability in a service, because the user experience is dominated by less reliable components like the cellular network or the device they are working with.<br><br>Eg: a user on a 99% reliable smartphone cannot tell the difference between 99.99% and 99.999% service reliability!<br><br><div><a href=""https://sre.google/sre-book/embracing-risk/"">https://sre.google/sre-book/embracing-risk/</a><br>ch 3</div>"	sre sre-book
What control loop is often used to manage release velocity wrt reliability?	"Error budgets, in the form of an agreed upon SLO.<br><br>For example, imagine that a service’s SLO is to successfully serve 99.999% of all queries per quarter. This means that the service’s error budget is a failure rate of 0.001% for a given quarter. If a problem causes us to fail 0.0002% of the expected queries for the quarter, the problem spends 20% of the service’s quarterly error budget.<br><br>For example, if product development wants to skimp on testing or increase push velocity and SRE is resistant, the error budget guides the decision.<br><br>An error budget aligns incentives and emphasizes joint ownership between SRE and product development. Error budgets make it easier to decide the rate of releases and to effectively defuse discussions about outages with stakeholders, and allows multiple teams to reach the same conclusion about production risk without rancor.<br><br><a href=""https://sre.google/sre-book/embracing-risk/"">https://sre.google/sre-book/embracing-risk/</a><br><div>ch 3</div>"	sre sre-book
What is an SLI? What can we do when the desired measure is hard to obtain or interpret? Example of such?	"SLI: A carefully defined quantitative measure of some aspect of the level of service that is provided. (eg: request latency, error rate, system throughput, availability, yield, durability)<br><br>Ideally, the SLI directly measures a service level of interest, but sometimes only a proxy is available because the desired measure may be hard to obtain or interpret (eg: client side vs server side latency)<br><br>ch 4:<br><a href=""https://sre.google/sre-book/service-level-objectives/"">https://sre.google/sre-book/service-level-objectives/</a><br><div><br></div>"	sre sre-book
What is an SLO? What's an example of 3 goals of SLOs (used by Evernote)?	"SLOs are the tool by which you measure your service’s reliability, and help determine what engineering work to prioritize.<br><br>SLO: A target value or range of values for a service level that is measured by an SLI. A natural structure for SLOs is thus SLI ≤ target, or lower bound ≤ SLI ≤ upper bound. An example of the latter is when you want to avoid over-reliance on a service, preventing users from believing that a service is more available than it actually is (eg: the Chubby planned global outage)<br><br>Choosing and publishing SLOs to users sets expectations about how a service will perform. This strategy can reduce unfounded complaints to service owners about, for example, the service being slow. Without an explicit SLO, users often develop their own beliefs about desired performance, which may be unrelated to the beliefs held by the people designing and operating the service.<br><br>Goals of SLOs in Evernote's use case:<br>Casting metrics in business terms and sharing a visible goal (an SLO!) between product and development will reduce a lot of misaligned expectations about reliability often seen in large companies.<br><br>1. Move engineering focus away from undifferentiated heavy lifting [in datacenters] and toward product engineering work that customers actually cared about.<br><br>2. Revise the working model of operations and software engineers to support an increase in feature velocity while maintaining overall quality of service.<br><br>3. Revamp how we look at SLAs to ensure that we increase focus on how failures impact our large global customer base.<br><br><a href=""https://sre.google/workbook/slo-engineering-case-studies/"">https://sre.google/workbook/slo-engineering-case-studies/</a><div>ch 4:<br><a href=""https://sre.google/sre-book/service-level-objectives/"">https://sre.google/sre-book/service-level-objectives/</a><br><br></div>"	sre sre-book sre-workbook
What is an SLA? How can you tell the difference between an SLO and an SLA? (and 1 example)	"SLA: an explicit or implicit contract with your users that includes consequences of meeting (or missing) the SLOs they contain.<br><br>An easy way to tell the difference between an SLO and an SLA is to ask ""what happens if the SLOs aren't met?"": if there is no explicit consequence, then you are almost certainly looking at an SLO. Google Search is an example of an important service that doesn’t have an SLA for the public.<br><br>SREs dont usually get involved with SLAs, but respond to missed SLOs and define SLIs.<br><br>ch 4:<br><a href=""https://sre.google/sre-book/service-level-objectives/"">https://sre.google/sre-book/service-level-objectives/</a><br>"	sre sre-book
What is white-box vs black-box monitoring? Examples of each? How do they compare?	"Black-box monitoring: Testing externally visible behavior as a user would see it.<br> - (eg: pingdom)<br> - system-oriented<br> - represents active, not predicted problems, ""the system isn't working correctly now""<br><br>White-box monitoring: Monitoring based on metrics exposed by the internals of the system.<br> - (Eg: logs, interfaces like the Java Virtual Machine Profiling Interface, or an HTTP handler that emits internal statistics)<br> - Depends on ability to inspect the innards of the systems with instrumentation<br> - allows detection of imminent problems, failures masked by retries, etc<br><br>When collecting telemetry for debugging, white-box monitoring is essential. If web servers seem slow on database-heavy requests, you need to know both how fast the web server perceives the database to be, and how fast the database believes itself to be. Otherwise, you can’t distinguish an actually slow database server from a network problem between your web server and your database.<br><br>Forces discipline to only nag a human when a problem is both already ongoing and contributing to real symptoms. On the other hand, for not-yet-occurring but imminent problems, black-box monitoring is fairly useless.<br><br><div>ch 6</div><div><a href=""https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals"">https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals</a><br></div>"	sre sre-book
What are the 4 golden signals? (wrt monitoring)	"Latency<br>&nbsp;&nbsp;&nbsp; The time it takes to service a request. It’s important to distinguish between the latency of successful requests and the latency of failed requests. For example, an HTTP 500 error triggered due to loss of connection to a database or other critical backend might be served very quickly; however, as an HTTP 500 error indicates a failed request, factoring 500s into your overall latency might result in misleading calculations. On the other hand, a slow error is even worse than a fast error! Therefore, it’s important to track error latency, as opposed to just filtering out errors.<br><br>Traffic / Volume<br>&nbsp;&nbsp;&nbsp; A measure of how much demand is being placed on your system, measured in a high-level system-specific metric. For a web service, this measurement is usually HTTP requests per second, perhaps broken out by the nature of the requests (e.g., static versus dynamic content). For an audio streaming system, this measurement might focus on network I/O rate or concurrent sessions. For a key-value storage system, this measurement might be transactions and retrievals per second.<br><br>Errors<br>&nbsp;&nbsp;&nbsp; The rate of requests that fail, either explicitly (e.g., HTTP 500s), implicitly (for example, an HTTP 200 success response, but coupled with the wrong content), or by policy (for example, ""If you committed to one-second response times, any request over one second is an error""). Where protocol response codes are insufficient to express all failure conditions, secondary (internal) protocols may be necessary to track partial failure modes. Monitoring these cases can be drastically different: catching HTTP 500s at your load balancer can do a decent job of catching all completely failed requests, while only end-to-end system tests can detect that you’re serving the wrong content.<br><br>Saturation / Utilization<br>&nbsp;&nbsp;&nbsp; How ""full"" your service is. A measure of your system fraction, emphasizing the resources that are most constrained (e.g., in a memory-constrained system, show memory; in an I/O-constrained system, show I/O). Note that many systems degrade in performance before they achieve 100% utilization, so having a utilization target is essential.<br><br>&nbsp;&nbsp;&nbsp; In complex systems, saturation can be supplemented with higher-level load measurement: can your service properly handle double the traffic, handle only 10% more traffic, or handle even less traffic than it currently receives? For very simple services that have no parameters that alter the complexity of the request (e.g., ""Give me a nonce"" or ""I need a globally unique monotonic integer"") that rarely change configuration, a static value from a load test might be adequate. As discussed in the previous paragraph, however, most services need to use indirect signals like CPU utilization or network bandwidth that have a known upper bound. Latency increases are often a leading indicator of saturation. Measuring your 99th percentile response time over some small window (e.g., one minute) can give a very early signal of saturation.<br><br>&nbsp;&nbsp;&nbsp; Finally, saturation is also concerned with predictions of impending saturation, such as ""It looks like your database will fill its hard drive in 4 hours.""<br><br><br>bonus: Tracking these for different endpoints, and consumers, of a service.<br><a href=""https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals"">https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals</a><br>"	sre sre-book
What are the 5 key ideas in devops? (3 bullets for the last one)	"1. No silos<br>2. Accidents are normal: often more profitable to focus on speedy recovery over preventing accidents.<br>3. Changes should be gradual: small and gradual is better, especially when coupled with automated testing and reliable rollback (eg: CI/CD)<br>4. Tooling and culture are interrelated: This is especially true when adopting a new way of working. (ie: ""culture eats strategy for breakfast"")<br>5. Measurement is crucial:<br>&nbsp;&nbsp; a. Establish the reality of what’s happening by means of objective measurement<br>&nbsp;&nbsp; b. Verify that you’re changing the situation as you expect<br>&nbsp;&nbsp; c. Create an objective foundation for conversations that different functions agree upon<br><br><div>ch 1:</div><div><a href=""https://sre.google/workbook/how-sre-relates/"">https://sre.google/workbook/how-sre-relates/</a><br></div>"	sre sre-workbook
What are the 7 SRE principles?	"1. Operations Is a Software Problem<br>SRE should therefore use software engineering approaches to solve that problem. This is across a wide field of view, encompassing everything from process and business change to similarly complicated but more traditional software problems, such as rewriting a stack to eliminate single points of failure in business logic.<br><br>2. Manage by SLOs (nothing can be 100% available)<br>Deciding on SLO targets requires strong collaboration from the business. SLOs have cultural implications as well: as collaborative decisions among stakeholders, SLO violations bring teams back to the drawing board, blamelessly.<br><br>3. Work to Minimize Toil<br>Any time spent on operational tasks means time not spent on project work—and project work is how we make our services more reliable and scalable. But note the ""wisdom of production"" that comes from toil.<br><br>4. Automate This Year’s Job Away (support 50% engineering time, although with Murphy-Beyer effect limitations)<br><br>5. Move Fast by Reducing the Cost of Failure<br>""The later in the product lifecycle a problem is discovered, the more expensive it is to fix""<br><a href=""https://www.agilemodeling.com/essays/costOfChange.htm"">https://www.agilemodeling.com/essays/costOfChange.htm</a><br><br>6. Share ownership w devs (""blur the lines"")<br>Ideally, both product development and SRE teams should have a holistic view of the stack—the frontend, backend, libraries, storage, kernels, and physical machine - and no team should jealously own single components.<br><br>7. Use the Same Tooling, Regardless of Function or Job Title<br>Eg: All Datacenters using the same hardware, Google's unified codebase for all devs, Borgman for monitoring, Chubby for locking, protobufs, etc.<br><a href=""https://sre.google/sre-book/production-environment/"">https://sre.google/sre-book/production-environment/</a><br><br>ch 1:<br><a href=""https://sre.google/workbook/how-sre-relates/"">https://sre.google/workbook/how-sre-relates/</a><br><br>"	sre sre-workbook
"What is the ""wisdom of production""? How does it related to toil?"	"By this phrase, we mean the wisdom you get from something running in production—the messy details of how it _actually_ behaves, and how software should _actually_ be designed, rather than a whiteboarded view of a service isolated from the facts on the ground. All of the pages you get, the tickets the team gets, and so on, are a direct connection with reality that should inform better system design and behavior.<br><br>Performing operational tasks (aka ""toil) does, however, by “the wisdom of production,” provide vital input into decisions. This work keeps us grounded by providing real-time feedback from a given system. Sources of toil need to be identifiable so you can minimize or eliminate them. However, if you find yourself in a position of operational underload, you may need to push new features and changes more often so that engineers remain familiar with the workings of the service you support.<br><br>ch 1:<br><a href=""https://sre.google/workbook/how-sre-relates/"">https://sre.google/workbook/how-sre-relates/</a><br><a href=""https://www.usenix.org/conference/srecon17europe/program/presentation/desai"">https://www.usenix.org/conference/srecon17europe/program/presentation/desai</a><br><br>"	sre sre-workbook
What is the Murphy-Beyer Effect? What insight can we derive from this observation?	"Over time, an SRE team winds up automating all that it can for a service, leaving behind things that can’t be automated. Other things being equal, this comes to dominate what an SRE team does unless other actions are taken.<br><br>Therefore, from that observation, we can derive the necessity of the hard limit for 50% engineering (or at least a limit on the toil).<br><br>Another nuance is the question of practically automatable versus theoretically automatable. If it is possible to do it, but you never get the time to do it, this rule will apply just as strongly. Something to bear in mind if teams get stuck in local minima.<br><br><a href=""https://hopes-and-strategies.blogspot.com/2017/12/the-murphy-beyer-effect.html"">https://hopes-and-strategies.blogspot.com/2017/12/the-murphy-beyer-effect.html</a><br>ch 1:<br><div><a href=""https://sre.google/workbook/how-sre-relates/"">https://sre.google/workbook/how-sre-relates/</a><br><br></div>"	sre sre-workbook
How are SRE and Devops similar (6 things)? How are they different (2 aphorisms)?	"1. Accept that change is necessary to improve.<br><br>2. Collaboration is front and center. Sharing strong values across the org helps climb out of team-based silos.<br><br>3. Tooling is critically important. Although API orientation for system management is a more important philosophy that will outlast any particular implementation of it.<br><br>4. Measurement is key:<br>For SREs, SLOs are dominant in determining the actions taken to improve the service (and SLOs require SLIs for measurement).<br>For DevOps, the act of measurement is often used to understand what the outputs of a process are, what the duration of feedback loops is, and so on.<br><br>5. Blameless Postmortems: Both practice them to offset unhelpful, adrenaline-laden reactions. Bad things happen occasionally and we have to talk about why.<br><br>6. Better velocity is the outcome for both.<br><br>Contrasts:<br>SRE brings an opinionated intellectual framework to the problem of how to run systems effectively.<br>2 aphorisms:<br>""struct SRE implements interface Devops""<br>""SRE believes in the same things as DevOps but for slightly different reasons""<br><br>ch: 1<br><a href=""https://sre.google/workbook/how-sre-relates/"">https://sre.google/workbook/how-sre-relates/</a><br>"	sre sre-workbook
2 reasons why it's recommended to treat an SLI as the ratio of two numbers? (eg: the number of good events divided by the total number of events)	"1. 0 to 100% is an intuitive scale.<br><br>2. In addition, making all of your SLIs follow a consistent style allows you to take better advantage of tooling: you can write alerting logic, SLO analysis tools, error budget calculation, and reports to expect the same inputs: numerator, denominator, and threshold. Simplification is a bonus here.<br><br>examples:<br>Number of successful HTTP requests / total HTTP requests (success rate)<br>Number of gRPC calls that completed successfully in &lt; 100 ms / total gRPC requests<br>Number of search results that used the entire corpus / total number of search results, including those that degraded gracefully<br>Number of “stock check count” requests from product searches that used stock data fresher than 10 minutes / total number of stock check requests<br>Number of “good user minutes” according to some extended list of criteria for that metric / total number of user minutes<br><br>ch 2:<br><a href=""https://sre.google/workbook/implementing-slos/"">https://sre.google/workbook/implementing-slos/</a><br>"	sre sre-workbook
What is SLI specification vs SLI implementation?	"SLI specification<br><br>&nbsp;&nbsp;&nbsp; The assessment of service outcome that you think matters to users, independent of how it is measured.<br>&nbsp;&nbsp;&nbsp; For example: Ratio of home page requests that loaded in &lt; 100 ms<br><br>SLI implementation<br><br>&nbsp;&nbsp;&nbsp; The SLI specification and a way to measure it.<br><br>&nbsp;&nbsp;&nbsp; For example:<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ratio of home page requests that loaded in &lt; 100 ms, as measured from the Latency column of the server log. This measurement will miss requests that fail to reach the backend.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ratio of home page requests that loaded in &lt; 100 ms, as measured by probers that execute JavaScript in a browser running in a virtual machine. This measurement will catch errors when requests cannot reach our network, but may miss issues that affect only a subset of users.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ratio of home page requests that loaded in &lt; 100 ms, as measured by instrumentation in the JavaScript on the home page itself, and reported back to a dedicated telemetry recording service. This measurement will more accurately capture the user experience, although we now need to modify the code to capture this information and build the infrastructure to record it—a specification that has its own reliability requirements.<br><br>As you can see, a single SLI specification might have multiple SLI implementations, each with its own set of pros and cons in terms of quality (how accurately they capture the experience of a customer), coverage (how well they capture the experience of all customers), and cost.<br><br><div>ch 2:<br><a href=""https://sre.google/workbook/implementing-slos/"">https://sre.google/workbook/implementing-slos/</a><br></div>"	sre sre-workbook
4 tips to when choosing which SLIs to start with?	"1. Choose one application for which you want to define SLOs. If your product comprises many applications, you can add those later.<br><br>2. Decide clearly who the “users” are in this situation. These are the people whose happiness you are optimizing.<br><br>3. Consider the common ways your users interact with your system—common tasks and critical activities.<br><br>4. Draw a high-level architecture diagram of your system; show the key components, the request flow, the data flow, and the critical dependencies. Group these components into request/pipeline/storage component categories (there may be some overlap and ambiguity; use your intuition and don’t let perfect be the enemy of the good).<br><br>You should think carefully about exactly what you select as your SLIs, but you also shouldn’t overcomplicate things. Especially if you’re just starting your SLI journey, pick an aspect of your system that’s relevant but easy to measure—you can always iterate and refine later.<br><br><div>ch 2:<br><a href=""https://sre.google/workbook/implementing-slos/"">https://sre.google/workbook/implementing-slos/</a><br></div>"	sre sre-workbook
What are 3 common types of components that we can abstract our system into?	"The easiest way to get started with setting SLIs is to abstract your system into a few common types of components. You can then use our list of suggested SLIs for each component to choose the ones most relevant to your service:<br><br>1. Request-driven<br>The user creates some type of event and expects a response. For example, this could be an HTTP service where the user interacts with a browser or an API for a mobile application.<br><br>2. Pipeline<br>A system that takes records as input, mutates them, and places the output somewhere else. This might be a simple process that runs on a single instance in real time, or a multistage batch process that takes many hours. Examples include:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A system that periodically reads data from a relational database and writes it into a distributed hash table for optimized serving<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A video processing service that converts video from one format to another<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A system that reads in log files from many sources to generate reports<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A monitoring system that pulls metrics from remote servers and generates time series and alerts<br><br>3. Storage<br>A system that accepts data (e.g., bytes, records, files, videos) and makes it available to be retrieved at a later date.<br><br><div>ch 2:<br><a href=""https://sre.google/workbook/implementing-slos/"">https://sre.google/workbook/implementing-slos/</a><br></div>"	sre sre-workbook
What are 7 common types of SLI specifications?	"1. Availability<br>The proportion of requests that resulted in a successful response.<br><br>2. Latency<br>The proportion of requests that were faster than some threshold.<br><br>3. Quality<br>If the service degrades gracefully when overloaded or when backends are unavailable, you need to measure the proportion of responses that were served in an undegraded state. For example, if the User Data store is unavailable, the game is still playable but uses generic imagery.<br><br>4. Freshness<br>The proportion of the data that was updated more recently than some time threshold. Ideally this metric counts how many times a user accessed the data, so that it most accurately reflects the user experience.<br><br>5. Correctness<br>The proportion of records coming into the pipeline that resulted in the correct value coming out.<br><br>6. Coverage<br>For batch processing, the proportion of jobs that processed above some target amount of data. For streaming processing, the proportion of incoming records that were successfully processed within some time window.<br><br>7. Durability (perhaps a variant on correctness)<br>The proportion of records written that can be successfully read. Take particular care with durability SLIs: the data that the user wants may be only a small portion of the data that is stored. For example, if you have 1 billion records for the previous 10 years, but the user wants only the records from today (which are unavailable), then they will be unhappy even though almost all of their data is readable.<br><br>(table 2-1)<br><br><br>Some of these SLIs may overlap: a request-driven service may have a correctness SLI, a pipeline may have an availability SLI, and durability SLIs might be viewed as a variant on correctness SLIs. We recommend choosing a small number (five or fewer) of SLI types that represent the most critical functionality to your customers.<br><br><div><br></div><div>ch 2:</div><div><a href=""https://sre.google/workbook/implementing-slos/#a-worked-example"">https://sre.google/workbook/implementing-slos/#a-worked-example</a><br></div>"	sre sre-workbook
What is a reporting period? What are 2 tradeoffs to consider when defining a reporting period?	"A reporting period (aka time window) is the specific time interval used when defining an SLO.<br><br>Rolling vs calendar:<br>Rolling windows are more closely aligned with user experience: if you have a large outage on the final day of a month, your user doesn’t suddenly forget about it on the first day of the following month. We recommend defining this period as an integral number of weeks so it always contains the same number of weekends.<br>Calendar windows are more closely aligned with business planning and project work.<br><br>Long vs short:<br>Shorter time windows allow you to make decisions more quickly: if you missed your SLO for the previous week, then small course corrections—prioritizing relevant bugs, for example—can help avoid SLO violations in future weeks.<br>Longer time periods are better for more strategic decisions: If you could choose only one of three large projects, would you be better off moving to a high-availability distributed database, automating your rollout and rollback procedure, or deploying a duplicate stack in another zone? You need more than a week's worth of data to evaluate large multiquarter projects; the amount of data required is roughly commensurate with the amount of engineering work being proposed to fix it.<br><br>A 4 week rolling window is generally recommended to start. Then complement this time frame with weekly summaries for task prioritization and quarterly summarized reports for project planning.<br><br>ch 2:<br><a href=""https://sre.google/workbook/implementing-slos/#choosing-an-appropriate-time-window"">https://sre.google/workbook/implementing-slos/#choosing-an-appropriate-time-window</a><br><br>"	sre sre-workbook
What are 3 aspects of an SLO needed to ensure stakeholder agreement?	"1. The product managers have to agree that this threshold is good enough for users—performance below this value is unacceptably low and worth spending engineering time to fix.<br>2. The product developers need to agree that if the error budget has been exhausted, they will take some steps to reduce risk to users until the service is back in budget (as discussed in Establishing an Error Budget Policy).<br>3. The team responsible for the production environment who are tasked with defending this SLO have agreed that it is defensible without Herculean effort, excessive toil, and burnout—all of which are damaging to the long-term health of the team and service.<br><br>Once all of these points are agreed upon, the hard part is done. You have started your SLO journey, and the remaining steps entail iterating from this starting point.<br><br>To defend your SLO you will need to set up monitoring and alerting (see Alerting on SLOs) so that engineers receive timely notifications of threats to the error budget before those threats become deficits.<br><br>ch 2:<br><a href=""https://sre.google/workbook/implementing-slos/"">https://sre.google/workbook/implementing-slos/</a><br>"	sre sre-workbook
What considerations should be made for each of the product manager, development team, and SRE team to test whether the SLOs are fit for purpose?	"1. If the SREs feel that the SLO is not defensible without undue amounts of toil, they can make a case for relaxing some of the objectives.<br><br>2. If the development team and product manager feel that the increased resources they’ll have to devote to fixing reliability will cause feature release velocity to fall below acceptable levels, then they can also argue for relaxing objectives. Remember that lowering the SLOs also lowers the number of situations to which the SREs will respond; the product manager needs to understand this tradeoff.<br><br>3. If the product manager feels that the SLO will result in a bad experience for a significant number of users before the error budget policy prompts anyone to address an issue, the SLOs are likely not tight enough.<br><br>If all three parties do not agree to enforce the error budget policy, you need to iterate on the SLIs and SLOs until all stakeholders are happy.<br><br><div>ch 2:<br><a href=""https://sre.google/workbook/implementing-slos/"">https://sre.google/workbook/implementing-slos/</a><br></div>"	sre sre-workbook
What common action items can be taken by a development team to enforce an error budget? (3 things)	"1. The development team gives top priority to bugs relating to reliability issues over the past four weeks.<br><br>2. The development team focuses exclusively on reliability issues until the system is within SLO. This responsibility comes with high-level approval to push back on external feature requests and mandates.<br><br>3. To reduce the risk of more outages, a production freeze halts certain changes to the system until there is sufficient error budget to resume changes.<br><br>Sometimes a service consumes the entirety of its error budget, but not all stakeholders agree that enacting the error budget policy is appropriate. If this happens, you need to return to the error budget policy approval stage.<br><br><div>ch 2:<br><a href=""https://sre.google/workbook/implementing-slos/"">https://sre.google/workbook/implementing-slos/</a><br></div>"	sre sre-workbook
What should an SLO document (6 things)? How often should it e reviewed?	"1. The authors of the SLO, the reviewers (who checked it for technical accuracy), and the approvers (who made the business decision about whether it is the right SLO).<br><br>2. The date on which it was approved, and the date when it should next be reviewed.<br><br>3. A brief description of the service to give the reader context.<br><br>4. The details of the SLO: the objectives and the SLI implementations.<br><br>5. The details of how the error budget is calculated and consumed.<br><br>6. The rationale behind the numbers, and whether they were derived from experimental or observational data. Even if the SLOs are totally ad hoc, this fact should be documented so that future engineers reading the document don’t make bad decisions based upon ad hoc data.<br><br>How often you review an SLO document depends on the maturity of your SLO culture. When starting out, you should probably review the SLO frequently—perhaps every month. Once the appropriateness of the SLO becomes more established, you can likely reduce reviews to happen quarterly or even less frequently.<br><br><div>ch 2:<br><a href=""https://sre.google/workbook/implementing-slos/"">https://sre.google/workbook/implementing-slos/</a><br></div>"	sre sre-workbook
What should an error budget document? (6 things) Why are error budgets useful?	"1. The policy authors, reviewers, and approvers<br>2. The date on which it was approved, and the date when it should next be reviewed<br>3. A brief description of the service to give the reader context<br>4. The actions to be taken in response to budget exhaustion<br>5. A clear escalation path to follow if there is disagreement on the calculation or whether the agreed-upon actions are appropriate in the circumstances<br>6. Depending upon the audience’s level of error budget experience and expertise, it may be beneficial to include an overview of error budgets.<br><br>Error budgets can be useful for quantifying these events—for example, ""this outage consumed 30% of my quarterly error budget,"" or ""these are the top three incidents this quarter, ordered by how much error budget they consumed.""<br><br><div>ch 2:<br><a href=""https://sre.google/workbook/implementing-slos/"">https://sre.google/workbook/implementing-slos/</a><br></div>"	sre sre-workbook
How can we measure user satisfaction with our service, to continuously improve SLO targets? (4 examples)	"1. You can count outages that were discovered manually, posts on public forums, support tickets, and calls to customer service.<br>2. You can attempt to measure user sentiment on social media.<br>3. You can add code to your system to periodically sample user happiness.<br>4. You can conduct face-to-face user surveys and samples.<br><br>The possibilities are endless, and the optimal method depends on your service. We recommend starting with a measurement that’s cheap to collect and iterating from that starting point. Asking your product manager to include reliability into their existing discussions with customers about pricing and functionality is an excellent place to start.<br><br><div>ch 2:<br><a href=""https://sre.google/workbook/implementing-slos/"">https://sre.google/workbook/implementing-slos/</a><br></div>"	sre sre-workbook
How can we audit whether our SLOs have adequate coverage (3 items)?	"1. Count your manually detected outages.<br><br>2. Count your support tickets.<br><br>3. Check that outages/incidents correlate with steep drops in error budget. Likewise, look at times when your SLIs indicate an issue, or your service fell out of SLO. Do these time periods correlate with known outages or an increase in support tickets? (can also use statistical analysis here, and/or plotting ""budget loss per day"" over ""tickets per day"")<br><br>If some of your outages and ticket spikes are not captured in any SLI or SLO, or if you have SLI dips and SLO misses that don’t map to user-facing issues, this is a strong sign that your SLO lacks coverage. This situation is totally normal and should be expected. Your SLIs and SLOs should change over time as realities about the service they represent change. Don’t be afraid to examine and refine them over time!<br><br><div>ch 2:<br><a href=""https://sre.google/workbook/implementing-slos/"">https://sre.google/workbook/implementing-slos/</a><br></div>"	sre sre-workbook
What are 4 courses of action to take if SLO lacks coverage?	"1. Change your SLO<br>&nbsp;&nbsp;&nbsp; If your SLIs indicated a problem, but your SLOs didn’t prompt anyone to notice or respond, you may need to tighten your SLO.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If the incident on that date was large enough that it needs to be addressed, look at the SLI values during the periods of interest. Calculate what SLO would have resulted in a notification on those dates. Apply that SLO to your historic SLIs, and see what other events this adjustment would have captured. It’s pointless to improve the recall of your system if you lower the precision such that the team must constantly respond to unimportant events.7<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Likewise, for false-positive days, consider relaxing the SLO.<br>&nbsp;&nbsp;&nbsp; If changing the SLO in either direction results in too many false positives or false negatives, then you also need to improve the SLI implementation.<br><br><br>2. Change your SLI implementation<br>&nbsp;&nbsp;&nbsp; There are two ways to change your SLI implementation: either move the measurement closer to the user to improve the quality of the metric, or improve coverage so you capture a higher percentage of user interactions. For example:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Instead of measuring success/latency at the server, measure it at the load balancer or on the client.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Instead of measuring availability with a simple HTTP GET request, use a health-checking handler that exercises more functionality of the system, or a test that executes all of the client-side JavaScript.<br><br>3. Institute an aspirational SLO<br><br>&nbsp;&nbsp;&nbsp; Sometimes you determine that you need a tighter SLO to make your users happy, but improving your product to meet that SLO will take some time. If you implement the tighter SLO, you’ll be permanently out of SLO and subject to your error budget policy. In this situation, you can make the refined SLO an aspirational SLO—measured and tracked alongside your current SLO, but explicitly called out in your error budget policy as not requiring action. This way you can track your progress toward meeting the aspirational SLO, but you won’t be in a perpetual state of emergency.<br><br>4. Iterate<br><br>&nbsp;&nbsp;&nbsp; There are many different ways to iterate, and your review sessions will identify many potential improvements. Pick the option that’s most likely to give the highest return on investment. Especially during the first few iterations, err on the side of quicker and cheaper; doing so reduces the uncertainty in your metrics and helps you determine if you need more expensive metrics. Iterate as many times as you need to.<br><br><div>ch 2:<br><a href=""https://sre.google/workbook/implementing-slos/"">https://sre.google/workbook/implementing-slos/</a><br></div>"	sre sre-workbook
What is the SLO decision matrix (3 key dimensions), and how should we perform against it?	This decision matrix provides suggested courses of action based on three key dimensions:<br><br>1. Performance against SLO (met/missed)<br>2. The amount of toil required to operate the service (hight/low)<br>3. The level of customer satisfaction with the service (high/low)<br><div><br></div> <pre>| SLO    | Toil | Customer satisfaction | Action                                                                                                              | |--------+------+-----------------------+---------------------------------------------------------------------------------------------------------------------| | Met    | Low  | High                  | Choose to (a) relax release and deployment processes and increase velocity,                                         | |        |      |                       | or (b) step back from the engagement and focus engineering time on services that need more reliability.             | | Met    | Low  | Low                   | Tighten SLO.                                                                                                        | | Met    | High | High                  | If alerting is generating false positives, reduce sensitivity.                                                      | |        |      |                       | Otherwise, temporarily loosen the SLOs (or offload toil) and fix product and/or improve automated fault mitigation. | | Met    | High | Low                   | Tighten SLO.                                                                                                        | | Missed | Low  | High                  | Loosen SLO.                                                                                                         | | Missed | Low  | Low                   | Increase alerting sensitivity.                                                                                      | | Missed | High | High                  | Loosen SLO.                                                                                                         | | Missed | High | Low                   | Offload toil and fix product and/or improve automated fault mitigation.                                             | </pre>Imagine that our example company had only one server failure in five years, but typically experiences two or three bad releases that require rollbacks per year. We can estimate that, on average, bad pushes cost twice as much error budget as database failures. The numbers prove that addressing the release problem provides much more benefit than investing resources in investigating the server failure.<br><br>If the service is running flawlessly and needs little oversight, then it may be time to move the service to a less hands-on tier of support. You might continue to provide incident response management and high-level oversight, but you no longer need to be as closely involved with the product on a day-to-day basis. Therefore, you can focus your efforts on other systems that need more SRE support.<br><div><br></div> 	sre sre-workbook
Assuming you have a healthy and mature error budget and SLO culture, what are some advanced techniques for improving and refining service reliability? (4 techniques)	"1. Modeling User Journeys<br><br>aka ""critical user journeys"", for example: Searching for a product, Adding a product to a shopping cart, then Completing a purchase<br><br>2. Grading Interaction Importance<br><br>Not all requests are created equal, so we may want to bucket certain classes of requests differently. Eg:<br><br>Premium vs Free customer tiers can have different availability SLOs.<br>Interactive vs CSV download requests can have different latency SLOs.<br><br>Can also bucket requests by individual customers as well.<br><br>3. Modeling Dependencies<br><br>While your prime concern is implementing a user-centric SLO that covers the entire stack, SLOs can also be a useful way to coordinate and implement reliability requirements between different components in the stack.<br><br>If a particular component has inherent reliability limitations, the SLO can communicate that limitation. If the user journey that depends upon it needs a higher level of availability than that component can reasonably provide, you need to engineer around that condition. You can either use a different component or add sufficient defenses (caching, offline store-and-forward processing, graceful degradation, etc.) to handle failures in that component.<br><br>Caveat for ""mathing"" your way out of these problems: You need 99.95% availability, but the service is only 99.9%. You may want to deploy it across multiple zones, thus giving it 99.9999% availability. But note that we're assuming that both services having an outage at the same time is driven by independent events. Usually there are common deps, failure domains, shared fate, and global control planes that can cause an outage in both at the same time.<br><br>There are two schools of thought regarding how an error budget policy should address a missed SLO when the failure is caused by a dependency that’s handled by another team:<br><br>a. Your team should not halt releases or devote more time to reliability, as your system didn’t cause the issue.<br>b. You should enact a change freeze in order to minimize the chances of future outages, regardless of the cause of that outage.<br><br>4. Experimenting with Relaxing Your SLOs<br><br>Do this thoughtfully, and only if you have error budget to burn.<br><br>You can run a helpful experiment to identify a relationship between a key business metric (e.g., sales) and a measurable technical metric (e.g., latency).<br><br>If it does, you have gained a very valuable piece of data you can use to make important engineering decisions for your service going forward.<br><br>But be careful not to misinterpret that data you get. For example, if you artificially slow your pages down by 50 ms and notice that no corresponding loss in conversions occurs, you might conclude that your latency SLO is too strict. However, your users might be unhappy, but simply lacking an alternative to your service at the moment. As soon as a competitor comes along, your users will leave. Be sure you are measuring the correct indicators, and take appropriate precautions.<br><br><div><br></div><div>ch 2:<br><a href=""https://sre.google/workbook/implementing-slos/"">https://sre.google/workbook/implementing-slos/</a><br></div>"	sre sre-workbook
Complete the sentence:<br><br>An error budget is a tool for ...?<br><br>An SLO is a tool for ...?	"1. SLOs are the tool by which you measure your service’s reliability.<br>2. Error budgets are a tool for balancing reliability with other engineering work, and a great way to decide which projects will have the most impact.<br><br><div>ch 2:<br><a href=""https://sre.google/workbook/implementing-slos/"">https://sre.google/workbook/implementing-slos/</a><br></div>"	sre sre-workbook
How can we characterize a traditional ops/dev split? What's the aphorism here?<br><br>How does SRE solve this? (2 things)	"How can we characterize a traditional ops/dev split? Why does SRE solve this?<br><br>An operations team protected the sanctity of the production environment, while a development team was tasked with developing new product features for customers. These objectives were usually in conflict: the dev team felt constrained by lengthy operational requirements, while the ops team became frustrated when new code introduced new issues in production.<br><br>Aphorism:<br>Instead of a ""You wrote it, you run it"" (development) model, or a ""You wrote it, we run it for you"" (operations) model, you can align on an SLO-centric SRE approach. The teams now have a common measure of success: removing the human interpretation of quality of service (QoS) has allowed both teams to maintain the same view and standards.<br><br>We were drawn to the SRE model because it:<br><br>1. fully embraces and accepts the differences between operations and development <br>2. while encouraging teams to work toward a common goal.<br><br><div>ch 3:<br><a href=""https://sre.google/workbook/slo-engineering-case-studies/"">https://sre.google/workbook/slo-engineering-case-studies/</a><br></div>"	sre sre-workbook
What's a simplified way for representing SLOs metrics? Why would we want to simplify this?<br><br>Example of 4 simplified metrics?	"Even though SLOs need to be internalized by the business owners of a service (eg: a PM), they may not be so intuitive. Here's an example of simplified metrics we might provide:<br><br>99.5%: Applications that are not used by store associates or an MVP of a new service<br>99.9%: Adequate for the majority of nonselling systems<br>99.95%: Selling systems (or services that support selling systems)<br>99.99%: Shared infrastructure services<br><br>Casting metrics in business terms and sharing a visible goal (an SLO!) between product and development will reduce a lot of misaligned expectations about reliability often seen in large companies.<br><br>ch 3:<br><a href=""https://sre.google/workbook/slo-engineering-case-studies/#ch03fn1-marker"">https://sre.google/workbook/slo-engineering-case-studies/#ch03fn1-marker</a><br><br>"	sre sre-workbook
What are 5 reasons to monitor, wrt SRE?	"1. Alert on conditions that require attention.<br>&nbsp;&nbsp; Something is broken, and somebody needs to fix it right now! Or, something might break soon, so somebody should look soon.<br><br>2. Investigate and diagnose those issues.<br>&nbsp;&nbsp; Conducting ad hoc retrospective analysis (i.e., debugging)<br>&nbsp;&nbsp; Our latency just shot up; what else happened around the same time?<br><br>3. Building dashboards<br>&nbsp;&nbsp; Display information about the system visually.<br>&nbsp;&nbsp; Dashboards should answer basic questions about your service, and normally include some form of the four golden signals (discussed in The Four Golden Signals).<br><br>4. Gain insight into trends in resource usage or service health for long-term planning.<br>&nbsp;&nbsp; How big is my database and how fast is it growing? How quickly is my daily-active user count growing?<br><br>5. Compare the behavior of the system before and after a change, or between two groups in an experiment.<br>&nbsp;&nbsp; Are queries faster with Acme Bucket of Bytes 2.72 versus Ajax DB 3.14? How much better is my memcache hit rate with an extra node? Is my site slower than it was last week?<br><br>The relative importance of these use cases might lead you to make tradeoffs when selecting or building a monitoring system.<br><br>System monitoring is also helpful in supplying raw input into business analytics and in facilitating analysis of security breaches. But those reasons are outside the focus of SRE.<br><br>re alerting: Unless you’re performing security auditing on very narrowly scoped components of a system, you should never trigger an alert simply because ""something seems a bit weird.""<br>Effective alerting systems have good signal and very low noise.<br><br>ch 4:<br><a href=""https://sre.google/workbook/monitoring/"">https://sre.google/workbook/monitoring/</a><div>ch 6:<br><a href=""https://sre.google/sre-book/monitoring-distributed-systems"">https://sre.google/sre-book/monitoring-distributed-systems</a><br></div>"	sre sre-book sre-workbook
What are 4 Desirable Features of a Monitoring Strategy? Why is each one important?	"1. Speed<br><br>There are 2 relevant needs here: _freshness_ of data, and _speed_ of data retrieval.<br>When data older than 4-5 mins, it can be hard to trace, and can cause you to incorrectly act on incorrect data.<br><br>Speed of data retrieval is mostly a problem when you’re querying vast amounts of data. It might take some time for a graph to load if it has to tally up a lot of data from many monitored systems. To speed up your slower graphs, it’s helpful if the monitoring system can create and store new time series based on incoming data; then it can precompute answers to common queries.<br><br>2. Calculations<br><br>The way you retain data depends on the use cases you need to support. You'll probably need to retain data over a multimonth time frame to support analyzing long-term trends. Usually summary data (ie: aggregated data that you can’t drill down into) is sufficient to facilitate growth planning.<br><br>Ideally event and resource consumption metrics should use monotonically increasing counters, allowing monitoring systems to calculate windowed functions over time (eg: rate of requests per second/week/month)<br><br>Support for a more complete range of statistical functions (eg: percentiles) can be useful because trivial operations may mask bad behavior.<br><br>You might also want to record your raw metric data in a separate system for offline analysis—for example, to use in weekly or monthly reports, or to perform more intricate calculations that are too difficult to compute in your monitoring system.<br><br>3. Interfaces<br><br>A robust monitoring system should allow you to concisely display time-series data in graphs, and also to structure data in tables or a range of chart styles. Your dashboards will be primary interfaces for displaying monitoring, so it’s important that you choose formats that most clearly display the data you care about. Some options include heatmaps, histograms, and logarithmic scale graphs.<br><br>You’ll likely need to offer different views of the same data based upon audience; high-level management may want to view quite different information than SREs. Be specific about creating dashboards that make sense to the people consuming the content. For each set of dashboards, displaying the same types of data consistently is valuable for communication.<br><br>You might need to graph information across different aggregations of a metric—such as machine type, server version, or request type—in real time. It’s a good idea for your team to be comfortable with performing ad hoc drill-downs on your data. By slicing your data according to a variety of metrics, you can look for correlations and patterns in the data when you need it.<br><br>4. Alerts<br><br>It’s helpful to be able to classify alerts: multiple categories of alerts allow for proportional responses. The ability to set different severity levels for different alerts is also useful: you might file a ticket to investigate a low rate of errors that lasts more than an hour, while a 100% error rate is an emergency that deserves immediate response.<br><br>The level of control you require over your system will dictate whether you use a third-party monitoring service or deploy and run your own monitoring system.<br><br><div>ch 4:<br><a href=""https://sre.google/workbook/monitoring/"">https://sre.google/workbook/monitoring/</a><br></div>"	sre sre-workbook
What is alert suppression, and 2 example use cases?	"Alert suppression functionality lets you avoid unnecessary noise from distracting on-call engineers. For example:<br><br>1. When all nodes are experiencing the same high rate of errors, you can alert just once for the global error rate instead of sending an individual alert for every single node.<br>2. When one of your service dependencies has a firing alert (e.g., a slow backend), you don’t need to alert for error rates of your service<br><br>You also need to be able to ensure alerts are no longer suppressed once the event is over.<br><br><div>ch 4:<br><a href=""https://sre.google/workbook/monitoring/"">https://sre.google/workbook/monitoring/</a><br></div>"	sre sre-workbook
What are 2 commons sources of monitoring data? What are their tradeoffs? And 2 less common sources?	"1. Metrics: less granular, but less ttl than logs. Alerts should be based off of metrics because it should all be configured in one place.<br><br>2. Logs: nearly always more accurate than metrics. When reporting isn’t time-sensitive, we often generate detailed reports using logs processing systems.<br><br>Other valuable sources of monitoring data are distributed tracing and runtime introspection.<br><br>Tips when writing logs and data:<br>In cases where it doesn't make sense to put a label on data, you can treat logs as a data store, and write a query for it as a runbook (eg: export HTTP status code as a metric label).<br><br>Instrumentation should write logs and metrics at the same time, with the same tooling and logic, to ensure uniform control surface.<br><div><br></div><div>ch 4:<br><a href=""https://sre.google/workbook/monitoring/"">https://sre.google/workbook/monitoring/</a><br></div>"	sre sre-workbook
What are 3 guidelines for managing a monitoring system? How does each contribute to treating it with appropriate care and attention?	"1. Treat Your Configuration as Code<br>Just like code, the monitoring system should have change history, versions, etc.<br><br>2. Encourage Consistency<br>Consider using the same monitoring framework, and a centralized dashboarding service, across all teams.<br>If possible, make basic monitoring coverage effortless. If all your services2 export a consistent set of basic metrics, you can automatically collect those metrics across your entire organization and provide a consistent set of dashboards. This approach means that any new component you launch automatically has basic monitoring. Many teams across your company—even nonengineering teams—can use this monitoring data.<br><br>3. Prefer Loose Coupling<br><br>Separate components should be in charge of collecting, storing, alerting, and visualizing your monitoring. Stable interfaces make it easier to swap out any given component for a better alternative.<br><br>Eg: Google was able to gradually switch from Borgmon to Monarch, without impacting graphs, which were all in a decoupled service Viceroy.<br><div><br></div><div>ch 4:<br><a href=""https://sre.google/workbook/monitoring/"">https://sre.google/workbook/monitoring/</a><br></div>"	sre sre-workbook
What other metrics should dashboards consider to help investigate an SLO violation? (4 metrics, one guideline)	"These metrics should provide reasonable monitoring that allows you to investigate production issues and also provide a broad range of information about your service.<br><br>1. Intended Changes<br><br>Add monitoring that informs you of any changes in production. To determine the trigger, we recommend the following:<br><br>a. Monitor the version of the binary.<br>b. Monitor the command-line flags, especially when you use these flags to enable and disable features of the service.<br>c. If configuration data is pushed to your service dynamically, monitor the version of this dynamic configuration.<br><br>If any of these pieces of the system aren’t versioned, you should be able to monitor the timestamp at which it was last built or packaged.<br><br>When you’re trying to correlate an outage with a rollout, it’s much easier to look at a graph/dashboard linked from your alert than to trawl through your CI/CD (continuous integration/continuous delivery) system logs after the fact.<br><br>2. Dependencies<br><br>Even if your service didn’t change, any of its dependencies might change or have problems, so you should also monitor responses coming from direct dependencies.<br><br>It’s reasonable to export the request and response size in bytes, latency, and response codes for each dependency. When choosing the metrics to graph, keep the four golden signals in mind. You can use additional labels on the metrics to break them down by response code, RPC (remote procedure call) method name, and peer job name.<br><br>Ideally, you can instrument the lower-level RPC client library to export these metrics once, instead of asking each RPC client library to export them<br><br>3. Saturation<br><br>Aim to monitor and track the usage of every resource the service relies upon. Some resources have hard limits you cannot exceed, like RAM, disk, or CPU quota allocated to your application. Other resources—like open file descriptors, active threads in any thread pools, waiting times in queues, or the volume of written logs—may not have a clear hard limit but still require management.<br><br>Depending on the programming language in use, you should monitor additional resources:<br><br>&nbsp;&nbsp;&nbsp; In Java: The heap and metaspace size, and more specific metrics depending on what type of garbage collection you’re using<br>&nbsp;&nbsp;&nbsp; In Go: The number of goroutines<br><br>Might also want to set up alerting for these resources, to avoid exceeding hard limits or perf degradation.<br><br>4. Status of Served Traffic<br><br>It’s a good idea to add metrics or metric labels that allow the dashboards to break down served traffic by status code (unless the metrics your service uses for SLI purposes already include this information). Here are some recommendations:<br><br>&nbsp;&nbsp;&nbsp; For HTTP traffic, monitor all response codes, even if they don’t provide enough signal for alerting, because some can be triggered by incorrect client behavior.<br>&nbsp;&nbsp;&nbsp; If you apply rate limits or quota limits to your users, monitor aggregates of how many requests were denied due to lack of quota.<br><br>Graphs of this data can help you identify when the volume of errors changes noticeably during a production change.<br><br><b>Guideline:</b> Implementing Purposeful Metrics<br><br>Each exposed metric should serve a purpose. Resist the temptation of exporting a handful of metrics just because they are easy to generate. Instead, think about how these metrics will be used.<br><br>When you write a postmortem, think about which additional metrics would have allowed you to diagnose the issue faster. (eg: ""debugging metrics"" - intended to provide insight about what is happening when alerts are triggered)<br><br><div>ch 4:<br><a href=""https://sre.google/workbook/monitoring/"">https://sre.google/workbook/monitoring/</a><br></div>"	sre sre-workbook
What are the three tiers for testing a monitoring env?	"1. Binary reporting: Check that the exported metric variables change in value under certain conditions as expected.<br><br>2. Monitoring configurations: Make sure that rule evaluation produces expected results, and that specific conditions produce the expected alerts.<br><br>3, Alerting configurations: Test that generated alerts are routed to a predetermined destination, based on alert label values.<br><br>If you can’t test your monitoring via synthetic means, or there’s a stage of your monitoring you simply can’t test, consider creating a running system that exports well-known metrics, like number of requests and errors. This will help validate that things are working. (It’s very likely that your alerting rules will not fire for months or years after you configure them, so you wanna make sure they work)<br><br>ch 4:<br><a href=""https://sre.google/workbook/monitoring/"">https://sre.google/workbook/monitoring/</a><br>"	sre sre-workbook
What are 4 dimensions to consider when evaluating an alerting strategy?	"Our goal is to be notified for a significant event: an event that consumes a large fraction of the error budget.<br><br>1. Precision<br><br>The proportion of events detected that were significant. Precision is 100% if every alert corresponds to a significant event. Note that alerting can become particularly sensitive to nonsignificant events during low-traffic periods (discussed in Low-Traffic Services and Error Budget Alerting).<br><br>2. Recall<br><br>The proportion of significant events detected. Recall is 100% if every significant event results in an alert.<br><br>3. Detection time<br><br>How long it takes to send notifications in various conditions. Long detection times can negatively impact the error budget.<br><br>4. Reset time<br><br>How long alerts fire after an issue is resolved. Long reset times can lead to confusion or to issues being ignored<br><br><div>ch 5:<br><a href=""https://sre.google/workbook/alerting-on-slos/"">https://sre.google/workbook/alerting-on-slos/</a><br></div>"	sre sre-workbook
"What is an ""error budget"" vs an ""error rate""?"	"The error budget gives the number of allowed bad events, and the error rate is the ratio of bad events to total events.<br><br>Often “error budgets” and “error rates” apply to all SLIs, not just those with “error” in their name.<br><br><div>ch 5:<br><a href=""https://sre.google/workbook/alerting-on-slos/"">https://sre.google/workbook/alerting-on-slos/</a><br><br></div>"	sre sre-workbook
What is burn rate? Why is it useful? (2 alert metrics)<div><br><br></div><br>Equation to calculate time it takes for an alert to fire? (given SLO, burn rate, error ratio, and alerting window)<br><div><br>Equation to calculate percent of error budget used? (given burn rate, alerting window, and monitoring period)<br></div><br><div><br><br>What does a burn rate of 1 mean?<br><div>How much error budget is spent with a burn rate of 36, alert window size of 1hr, and a monitoring period of 30 days?<br></div></div><br><br>	Burn rate is how fast, relative to the SLO, the service consumes the error budget.<br>This enables us to create an alert with good detection time and high precision.<br><br>Assuming that our SLO is 99.9% and has a time window of 30 days:<br><div><br></div><div><br></div><br><pre>| Burn rate | Error rate for a 99.9% SLO | Time to exhaustion |<br>|-----------+----------------------------+--------------------|<br>|         1 |                       0.1% | 30 days            |<br>|         2 |                       0.2% | 15 days            |<br>|        10 |                         1% | 3 days             |<br>|      1000 |                       100% | 43 minutes         |<br></pre><br>Eg: a burn rate of 1 means that it’s consuming error budget at a rate that leaves you with exactly 0 budget at the end of the SLO’s time window<br><br>To calculate time it takes for an alert to fire:<br><br>(1 - SLO)/error_ratio * alert_window_size * burn_rate<br><br><br>To calculate percent error budget spend:<br>spend = (burn_rate * alert_window_size) / period<br><br>spend = 36 * 1hr / (30 * 24 hours) = 6 / (30 * 4) = 6/120 = 1/20 = 5%<br><br><div><br></div><div><br></div><br>	sre sre-workbook
"For each of the following alerting strategies, name their pros/cons wrt precision/recall/detection/reset times, and why none of them are viable.<br><br>Assume an SLO for 99.9% availability.<br><br>1. ""Target Error Rate ≥ SLO Threshold"":<br><br><pre>- alert: HighErrorRate<br>  expr: job:slo_errors_per_request:ratio_rate10m{job=""myjob""} &gt;= 0.001<br></pre><br><br>2. What if we increase the alert window?<br><br><pre>- alert: HighErrorRate<br>   expr: job:slo_errors_per_request:ratio_rate36h{job=""myjob""} &gt; 0.001</pre><br>3. What if we increase the alert duration?<br><br><pre>- alert: HighErrorRate<br>    expr: job:slo_errors_per_request:ratio_rate1m{job=""myjob""} &gt; 0.001<br>    for: 1h<br></pre><br>"	"Note that none of these are viable because we need to alert on burn rate to create an alert with good detection and high precision.<br><br>1. For the most trivial solution, you can choose a small time window (for example, 10 minutes) and alert if the error rate over that window exceeds the SLO.<br><br>pros:<br>Detection time is good.<br><br>This alert fires on any event that threatens the SLO, exhibiting good recall.<br><br>cons:<br>Precision is low: The alert fires on many events that do not threaten the SLO.<br><br>2. We can build upon the preceding example by changing the size of the alert window to improve precision. By increasing the window size, you spend a higher budget amount before triggering an alert.<br><br>pros:<br>Detection time is still good.<br><br>Better precision than the previous example: by ensuring that the error rate is sustained for longer, an alert will likely represent a significant threat to the error budget.<br><br>cons:<br>Very poor reset time: In the case of 100% outage, an alert will fire shortly, but continue to fire for the remaining window.<br><br>Calculating rates over longer windows can be expensive in terms of memory or I/O operations, due to the large number of data points.<br><br>3. Most monitoring systems allow you to add a duration parameter to the alert criteria so the alert won’t fire unless the value remains above the threshold for some time. You may be tempted to use this parameter as a relatively inexpensive way to add longer windows.<br><br>pros:<br>Alerts can be higher precision. Requiring a sustained error rate before firing means that alerts are more likely to correspond to a significant event.<br><br>cons:<br>Poor recall and poor detection time: Because the duration does not scale with the severity of the incident, a 100% outage alerts after one hour, the same detection time as a 0.2% outage. The 100% outage would consume 140% of the 30-day budget in that hour.<br><br>If the metric even momentarily returns to a level within SLO, the duration timer resets. An SLI that fluctuates between missing SLO and passing SLO may never alert.<br><br>ch 5:<br><a href=""https://sre.google/workbook/alerting-on-slos/#ways-to-alert-on-significant-events"">https://sre.google/workbook/alerting-on-slos/#ways-to-alert-on-significant-events</a><br><br>"	sre sre-workbook
"For each of the following alert strategies, explain their pros/cons wrt precision/recall/detection/reset times, and why they're viable strategies:<div><br></div><div>1. Alerting on a burn rate of 36, 5% error budget spend, with alert window of 1hr and SLO 99.9%:<br><br><br> <pre>- alert: HighErrorRate   expr: job:slo_errors_per_request:ratio_rate1h{job=""myjob""} &gt; 36 * 0.001 </pre> <br>2. Multiple burn rate alerts:<br><br><br> <pre>expr: (         job:slo_errors_per_request:ratio_rate1h{job=""myjob""} &gt; (14.4*0.001)       or         job:slo_errors_per_request:ratio_rate6h{job=""myjob""} &gt; (6*0.001)       ) severity: page  expr: job:slo_errors_per_request:ratio_rate3d{job=""myjob""} &gt; 0.001 severity: ticket </pre> <br>3. Multiwindow, Multi-Burn-Rate Alerts<br><br><br> <pre>expr: (         job:slo_errors_per_request:ratio_rate1h{job=""myjob""} &gt; (14.4*0.001)       and         job:slo_errors_per_request:ratio_rate5m{job=""myjob""} &gt; (14.4*0.001)       )     or       (         job:slo_errors_per_request:ratio_rate6h{job=""myjob""} &gt; (6*0.001)       and         job:slo_errors_per_request:ratio_rate30m{job=""myjob""} &gt; (6*0.001)       ) severity: page  expr: (         job:slo_errors_per_request:ratio_rate24h{job=""myjob""} &gt; (3*0.001)       and         job:slo_errors_per_request:ratio_rate2h{job=""myjob""} &gt; (3*0.001)       )     or       (         job:slo_errors_per_request:ratio_rate3d{job=""myjob""} &gt; 0.001       and         job:slo_errors_per_request:ratio_rate6h{job=""myjob""} &gt; 0.001       ) severity: ticket </pre> <br></div>"	"1. By keeping the alert window fixed at one hour and deciding that a 5% error budget spend is significant enough to notify someone, you can derive the burn rate of 36 to use for the alert.<br><br>pros:<br>Good precision: This strategy chooses a significant portion of error budget spend upon which to alert.<br><br>Shorter time window, which is cheaper to calculate.<br><br>Good detection time.<br><br>cons:<br>Low recall: A 35x burn rate never alerts, but consumes all of the 30-day error budget in 20.5 hours.<br><br>Reset time: 58 minutes is still too long.<br><br>2. Multiple burn rates allow you to adjust the alert to give appropriate priority based on how quickly you have to respond. If an issue will exhaust the error budget within hours or a few days, sending an active notification is appropriate. Otherwise, a ticket-based notification to address the alert the next working day is more appropriate.<br><br><br> <pre>| SLO budget consumption | Time window | Burn rate | Notification | |------------------------+-------------+-----------+--------------| |                     2% | 1hr         |      14.4 | page         | |                     5% | 6hr         |         6 | page         | |                    10% | 3 days      |         1 | ticket       | </pre> <br>pros:<br>Ability to adapt the monitoring configuration to many situations according to criticality: alert quickly if the error rate is high; alert eventually if the error rate is low but sustained.<br><br>Good precision, as with all fixed-budget portion alert approaches.<br><br>Good recall, because of the three-day window.<br><br>Ability to choose the most appropriate alert type based upon how quickly someone has to react to defend the SLO.<br><br>cons:<br>More numbers, window sizes, and thresholds to manage and reason about.<br><br>An even longer reset time, as a result of the three-day window.<br><br>To avoid multiple alerts from firing if all conditions are true, you need to implement alert suppression. For example: 10% budget spend in five minutes also means that 5% of the budget was spent in six hours, and 2% of the budget was spent in one hour. This scenario will trigger three notifications unless the monitoring system is smart enough to prevent it from doing so.<br><br>3. Notify us only when we’re still actively burning through the budget—thereby reducing the number of false positives. To do this, we need to add another parameter: a shorter window to check if the error budget is still being consumed as we trigger the alert.<br><br> <pre>| Severity | Long window | Short window | Burn rate | Error budget consumed | |----------+-------------+--------------+-----------+-----------------------| | Page     | 1hr         | 5 mins       |      14.4 |                    2% | | Page     | 6hr         | 30 mins      |         6 |                    5% | | Ticket   | 3days       | 6hr          |         1 |                   10% | </pre> <br>pros:<br>A flexible alerting framework that allows you to control the type of alert according to the severity of the incident and the requirements of the organization.<br><br>Good precision, as with all fixed-budget portion alert approaches.<br><br>Good recall, because of the three-day window.<br><br>cons:<br>Lots of parameters to specify, which can make alerting rules hard to manage.<br><br>ch 5:<br><a href=""https://sre.google/workbook/alerting-on-slos/#ways-to-alert-on-significant-events"">https://sre.google/workbook/alerting-on-slos/#ways-to-alert-on-significant-events</a><br><br>"	sre sre-workbook
What technique can we use to scale our alerting strategy across many SLOs?	We can avoid having to supply alert window and burn rate parameters independently for each surface by grouping request types into the following buckets:<br><br>CRITICAL: For request types that are the most important, such as a request when a user logs in to the service.<br><br>HIGH_FAST: For requests with high availability and low latency requirements. These requests involve core interactive functionality, such as when a user clicks a button to see how much money their advertising inventory has made this month.<br><br>HIGH_SLOW: For important but less latency-sensitive functionality, such as when a user clicks a button to generate a report of all advertising campaigns over the past few years, and does not expect the data to return instantly.<br><br>LOW: For requests that must have some availability, but for which outages are mostly invisible to users—for example, polling handlers for account notifications that can fail for long periods of time with no user impact.<br><br>NO_SLO: For functionality that is completely invisible to the user—for example, dark launches or alpha functionality that is explicitly outside of any SLO.<br><br><div> <pre>| Request class | Availability | Latency @ 90%6 | Latency @ 99% | |---------------+--------------+----------------+---------------| | CRITICAL      |       99.99% | 100 ms         | 200 ms        | | HIGH_FAST     |        99.9% | 100 ms         | 200 ms        | | HIGH_SLOW     |        99.9% | 1,000 ms       | 5,000 ms      | | LOW           |          99% | None           | None          | | NO_SLO        |         None | None           | None          | </pre> <br></div>	sre sre-workbook
Why doesn't the multi-window, multi-burn-rate approach work for low traffic services? What are 4 options for handling this?	"It’s harder to automatically distinguish unimportant events in low-traffic services. For example, if a system receives 10 requests per hour, then a single failed request results in an hourly error rate of 10%. For a 99.9% SLO, this request constitutes a 1,000x burn rate and would page immediately, as it consumed 13.9% of the 30-day error budget. This scenario allows for only seven failed requests in 30 days. Single requests can fail for a large number of ephemeral and uninteresting reasons that aren’t necessarily cost-effective to solve in the same way as large systematic outages.<br><br>1. Generating fake traffic, when doing so is possible and can achieve good coverage<br><br>Downsides:<br>Only a small portion of user request types can usually be synthesized.<br>If an issue affects real users but doesn’t affect artificial traffic, the successful artificial requests hide the real user signal, so you aren’t notified that users see errors.<br><br>2. Combining services<br>For this approach to work, the services must be related in some way—you can combine microservices that form part of the same product, or multiple request types handled by the same binary.<br><br>Downside: complete failure of an individual service might not count as a significant event.<br><br>3. Making service and infra changes<br><br>Change service to reduce impact of a single failed request:<br><br>a. Modify the client to retry, with exponential backoff and jitter<br>b. Set up fallback paths that capture the request for eventual execution, which can take place on the server or on the client.<br><br>These changes are useful for high-traffic systems, but even more so for low-traffic systems: they allow for more failed events in the error budget, more signal from monitoring, and more time to respond to an incident before it becomes significant.<br><br>4. Setting SLO thresholds commensurate with the actual impact of a failed request<br><br>If a small number of errors causes you to lose error budget, do you really need to page an engineer to fix the issue immediately? If not, users would be equally happy with a lower SLO. With a lower SLO, an engineer is notified only of a larger sustained outage<br><br>Note that changing the SLO affects other aspects of the system, such as expectations around system behavior and when to enact the error budget policy. These other requirements may be more important to the product than avoiding some number of low-signal alerts.<br><div><br></div><div>ch 5:<br><a href=""https://sre.google/workbook/alerting-on-slos/"">https://sre.google/workbook/alerting-on-slos/</a><br><br></div>"	sre sre-workbook