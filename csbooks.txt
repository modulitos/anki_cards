what are the basics of the von neumann model of computing? (3 steps)	A program does one simple thing: it executes instructions.<br><br>Many millions (and these days, billions) of times every second, the processor fetches an instruction from memory, decodes it (ie: figures out which instruction this is) and executes it (ie it does the thing it's supposed to do, like add 2 numbers, access memory, check a condition, jump to a function, etc). After it is done with this instruction, the processor moves on to the next instruction, and so on, until the program finally completes.<br>	chapter2 OSTEP
what is (virtual) address space? What does it consist of? (2.2, 13.3)	"It's the running program's view of memory in the system.<br>aka ""address space"", each process has it's own private address space, which the OS maps into the physical memory of the machine. A memory reference within one running program does not affect the address space of the other processes (or the OS itself); as far as the running program is concerned, it has the memory all to itself.<br>In reality, it's a shared resource, managed by the OS, which accomplishes this by virtualization.<br><br>Consists of code, stack, and heap, on a basic level.<br>"	chapter13 chapter2 OSTEP
what is a thread? (2.3)	A thread is like a function running within the same memory space as other functions, with more than one of them active at a time.	chapter2 OSTEP
what is a file system? and files? Why aren't files virtualized by the OS? (2.4)	A file system is software that manages the disk, and is responsible for storing files the user creates in a reliable and efficient manner on the disks of the system.<br>Unlike the abstractions provided by the OS for the CPU and memory, the OS does not create a private, virtualized disk for each application. Rather, it is assumed that often times, users will want to share information that is in files (like data and executables)<br>	chapter2 OSTEP
what is journaling (aka copy-on-write)? (2.4)	Helps handle system crashes during writes by carefully ordering writes to disk to ensure that if a failure occurs during a write sequence, the system can recover to a reasonable state afterwards.	chapter2 OSTEP
what are 6 design goals in designing + implementing an OS? (2.5)	1. perf - minimize the overhead of the OS<br>2. protection (and isolation) - ensure one malicious or accidental bad behavior doesn't affect the other programs<br>3. reliability - runs non-stop, and when it fails, it fails all apps as well<br><br>secondary goals:<br>4. energy-efficiency<br>5. security - extension of protection, esp. related to networking<br>6. mobility - running on different (and smaller) devices<br><br>	chapter2 OSTEP
Difference between syscall and procedural call? What is the key difference? (2.6)	syscall provides OS routines as a library (accessible via a proc call), enabling the ability to add a special pair of hardware instructions and hardware state to make the transition into the OS a more formal, controlled process.<br>Key difference is that when initiated, the hw transfers control (ie jumps) to a pre-specified trap handler (set up by the OS previously), while simultaneously raising the hw privilege level from user mode to kernel mode.<br><br>	chapter2 OSTEP
How does the OS allow users to run as many concurrent processes as they like? What's the downside if they run too many?	By running one process, then stopping it and running another, the OS promotes the illusion that many virtual CPU's exist, although there's only one (or a few).<br>The potential cost is perf - where each will run more slowly if the CPU must be shared.<br>	chapter4 OSTEP
What parts of the machine state can a process read or update while running? (4.1)	memory (address space)<br>registers, including:<br> - the program counter (PC), aka instruction counter (IC)<br> - the stack pointer and associated function pointer<br>storage devices<br>	chapter4 OSTEP
5 apis of a proc that an OS must provide? (4.2)	1. create<br>2. destroy<br>3. wait - wait for a proc to stop running<br>4. misc control - other than killing or waiting, most OS's provide a way to suspend and resume it as well<br>5. status - how long the process has run for; what state it's in<br>	chapter4 OSTEP
What are the 3 proc states? How do they get to each state? (4.4)	"1. Running - executing instructions on a processor<br>2. Ready - ready to run, but for some reason the OS is not running it at the given moment<br>3. Blocked - process has performed some kind of operation that makes it not ready to run until some other event takes places (eg: waiting on IO)<br>additional states:<br>4. Initial - still being created<br>5. Final - exited, but not cleaned up<br><br>scheduling takes a process from ""ready"" to ""running""<br>de-scheduling takes a process from ""running"" to ""ready""<br>IO-initiate takes from ""ready"" to ""blocked""<br>IO-done takes from ""blocked"" to ""ready""<br>"	chapter4 OSTEP
What is a proc list? What is it used for? (4.5)	<br>Used by OS to track which proc is running, and all ready/blocked processes, to help the scheduler.<br><br>	chapter4 OSTEP
What is a shell? What are the 4 steps and 3 syscalls it leverages? (5.4, 5 summary)	A shell is just a user program, with a prompt for user input.<br><br>It does the following:<br>1. finds where in the filesystem the executable resides<br>2. calls fork() create a new child process to run the command<br>3. calls exec() to run it<br>4. calls wait() when done, printing out the prompt<br>	chapter5 OSTEP
What do trap instructions do (3 things)? (6.5)	The trap instruction:<br>1. saves register state carefully<br>2. changes the hardware status to kernel mode<br>3. jumps into the OS to a pre-specified destination: the trap table<br>	chapter6 OSTEP
What is limited direct execution? (3 things)	LDE is <br>1. a protocol <br>2. which runs programs efficiently <br>3. but without loss of OS control. <br><br>Does this by leveraging trap tables that must be set up by the OS at boot time, to make sure they cannot be readily modified by user programs.	chapter6 OSTEP
What is cooperative approach for process switching? when does it switch contexts? (6.3)	"When the OS trusts the processes of the system to behave reasonably. Processes that run for too long are assumed to periodically give up the CPU (via syscalls) so that the OS can decide to run some other task.<br>OS's like this often include an explicit ""yield"" call, which does nothing except transfer control to the OS so it can run other processes.<br>"	chapter6 OSTEP
In a non-cooperative (pre-emptive) approach, how can the OS ensure a rogue proc doesn't take over, thus maintaining control of the CPU? What protocol does this use? (6.3)	A timer interrupt gives the OS the ability to run again on a CPU even if processes act in a non-cooperative fashion. This is a hardware feature essential for maintaining control.<br><br>This uses the limited direct execution protocol	chapter6 OSTEP
What is workload wrt process schedulers? What's this term useful for? (7.1)	Refers to all the processes running in a system, useful for building policies. The more you know about workload, the more fine-tuned your policy can be.	chapter7 OSTEP
What is the convoy effect? (7.3)	A number of relatively short potential customers of a resource get queued behind a heavyweight resource consumer. (like the line at a grocery store)<br><br>Is a problem when FIFO scheduling	chapter7 OSTEP
What is response time, why does it matter? (7.6)	Time from when a job first arrives in a system until it is scheduled.<br><br>Tresponse = Tscheduled - Tarrive<br>	chapter7 OSTEP
What is the tradeoff of having a short vs long time slice? (7.7)	The shorter it is, the better the response time. But if too short, the cost of context switching will start to dominate overall perf.<br>Thus, it should be long enough to amortize the cost of swtiching without making it so long that the system is no longer resonsive.<br>	chapter7 OSTEP
What's the tradeoff between turnaround time (TAT) and RR?	RR is one of the worst for process turn-around-time (TAT). On the other hand, it's great for response time.	chapter7 OSTEP
What is overlap? (7.8)	CPU being used by one process, while waiting on the IO of another, making the system better utilized.	chapter7 OSTEP
What are the 5 rules of mlfq? How to prevent gaming the CPU? (8.4)	1. If priority(A) &gt; priority(B), then A runs (B doesn't)<br>2. If priority(A) == priority(B), then A and B run in RR<br>3. When a job enters the system, it is placed at the highest priority (topmost queue)<br>4. Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), it's priority is reduced (moves down a queue)<br>5. After some period S, move all jobs to the topmost queue (priority boost), to prevent lots of short running jobs from starving a long+running job<br><br>Rule 4 prevents gaming the CPU, by not rewarding jobs that give up the CPU right before their time slice ends. (previous rules would only downgrade priority after a timeslice finishes)<br><br>https://pages.cs.wisc.edu/~remzi/OSTEP/cpu-sched-mlfq.pdf<br>	chapter8 OSTEP
What is prop share (aka fair share) scheduling, and an example of one, and its advantages? (9.0	"Instead of optimizing for TAT or response time, a scheduler might instead try to guarantee that each job obtain a certain percentage of CPU time.<br>Eg: Lottery scheduling, where ""tickets"" are allotted to each process, and a lottery is held to determine which one to run next. The advantage is that it's easy to add a new process - just update the total number of lottery tickets.<br><br>"	chapter9 OSTEP
What is Linux's CFS, and how does it use vruntime, sched_latency, and nice values?	CFS is a bit like weighted RR with dynamic time slaces, but built to scale and perform well under load.<br>vruntime - each process accumulates vruntime proportional to physical time. When a scheduling decision occurs, CFS will pick the process with the lowest vruntime.<br>sched_latency - determines how long a process should run before considering a switch (thus dynamically determining the time slice)<br>niceness - can be set anywhere from -20 to +19 for a process, with a default of 0. The lower it is, the higher priority of the process.<br>	chapter9 OSTEP
What are the 2 primary purposes of an OS?<br><br>3 abstractions provided by OS over which hardware? Why is it so paranoid?	"1.To provide apps w simple uniform mechanisms for manipulating complicated and wildly different low-level hardware devices <br>2. To protect the hardware from misuse by runaway apps<br><br>3 abstractions (CSAPP 1.9.3):<br>1. files abstraction for IO devices<br>2. virtual memory abstraction for both main memory and disks<br>3. processes abstraction for processor, main memory, and IO devices<br><br>The OS is fairly paranoid. It wants to make sure it stays in charge of the machine. While it wants a program to run as efficiently as possible (and hence the whole reasoning behind LDE), the OS also wants to be able to say ""Ah! Not so fast my friend"" in case of an errant or malicious process. Paranoia rules the day, and certainly keeps the OS in charge of the machine. Perhaps that's why we think of the OS as a resource manager.<br><br>"	chapter11 CSAPP OSTEP
What are the 3 goals of a VM system? (13.4)	Transparency - the program shouldn't be aware that the memory is virtualized; program behaves like its own private memory. The OS/hw does all the work to multiplex memory among many jobs.<br><br>Efficiency - in both time (ie not slowing down programs) and space (ie not using too much memory for support). hw features and TLB's are essential here.<br><br>Protection - enables us to deliver the property of isolation among processes.<br><br>	chapter13 OSTEP
What is interposition, and how is it used in a VM system? (15.2)	"Just like in LDE, the OS delivers efficient virtualization by mostly ""getting out of the way"". But it still interposes at critical points in time, to ensure it maintains control over the hardware.<br><br>We want programs to use address spaces in whatever way they like, while ensuring the accesses stay within the bounds of the address space.<br><br>TLB's enable this by interpositioning at the hardware space, to provide efficiency and flexibility.<br><br>It's all performed in a way that's transparent to the process; the process has no idea that its addresses are being translated.<br>"	chapter15 OSTEP
What are a couple techniques for supporting a process that contains a large address space? (16.1)	segmentation and paging	chapter16 OSTEP
What is segmentation? What are some problems that arise, and how are those problems solved? (16.6)	Segmentation builds more effective virtualization of memory. Helps us support sparse address spaces, by avoiding the huge potential waste of memory between logical segments of address space. It's also fast, as it's base/bounds arithmetic is well suited to hardware.<br><br>However, external fragmentation becomes a problem as the memory gets chopped up into odd-sized pieces, which is a fundamental problem that's hard to avoid. It's also not flexible enough to support our fully generalized sparse address space. (eg: having a large but sparsely used heap, which must reside in memory in order to be accessed).<br><br>We can solve these problems with paging.<br>	chapter16 OSTEP
What is external vs internal fragmentation wrt a processor's address space? What are its causes, and how to address it? (17.1)	External Fragmentation: the free space gets chopped up into little pieces of different sizes and is thus fragmented; causing subsequent requests to fail in finding a fit.(can be caused by sparse allocation, and addressed by paging)<br><br>Internal Fragmentation (15.6): the space inside the allocated unit is not all used (ie is fragmented) and thus wasted. This also happens when an allocator hands out chunks of memory bigger than requested (the waste occurs inside the allocated unit). Can be caused by allocating larger pages, (or Buddy Allocation wrt malloc).<br> <br><br>	chapter17 OSTEP
What is a free list, and what's it used for? What are 3 mechanisms it supports? (17.1-2)	Contains a set of elements that describe the free space still remaining on the heap.<br>Supports splitting, coalescing, and growing (by requesting more space from the OS).<br>Used by a memory allocator (eg: malloc) for efficiency.<br><br>	chapter17 OSTEP
When tracking allocated regions in a free list, how is a header block used? What's stored in the header? (17.2)	"When freeing memory (via the ""free(void *ptr)"" interface), the malloc lib must quickly determine the size of the region of memory being freed and incorportate the space back into the free list.<br>Header block is used for this, and in addition to the allocated region, it can store additional pointers to speed up deallocation, and magic numbers for addtl integrity checking.<br>Each node in free list must contain extra space for this header block.<br><br>"	chapter17 OSTEP
What are 4 basic malloc strategies, along w 2 more advanced ones? (17.3-4)	"BEST - tries to reduce wasted space by returning a block that is close to what the user asked. However, exhaustive search can be expensive.<br>WORST - tries to leave big chunks free instead of lots of small chunks that can arise from the best-fit approach. Still requires exhaustive search. Most studies shows this performs badly.<br>FIRST FIT - simply finds the first block that is big enough. Advantage is speed, but sometimes pollutes the beginning of the list with small objects. How the allocator manages the list order becomes an issue here.<br>NEXT FIT - like ""first fit"", but keeps an extra pointer to the location within the list where one was looking last. Idea is to spread the searches for free space throughout the list more uniformly, thus avoiding splinter of the beginning of the list.<br><br>advanced:<br>Segregated Lists - keep a separate list for serving popular requests; all other requests are forwarded to a more general memory allocator. Fragmentation is much less of a concern by having a chunk of memory dedicated for one particular size of requests<br><br>Buddy Allocation - recursively divides spaces until a block that is big enough to accommodate the request is found. When a block is freed, the ""buddy"" block is checked, and if free, they get coalesced. Happens recursively. Can increase internal fragmentation, but ensures that all blocks of size 2^n.<br><br>"	chapter17 OSTEP
What is a page table? What does it store? How many tables are needed per process? (18.1,3,5)	"Stores virtual-to-physical address translations, thus letting the system know where each page of an address space actually resides in physical memory.<br>Because each address space requires such translations, in general there is one page table per process in the system. Exact structure of page table is OS-specific.<br><br>Maps virtual page number (VPN) in the process to a ""physical frame number"" (PFN) (aka ""physical page number"") on the system.<br>The page table tells you, for each virtual page number (VPN) of the address space, that the virtual page is mapped to a particular physical frame number (PFN) and thus valid, or not valid.<br><br>offset = VA &amp; OFFSET_MASK<br>PhysAddr = (PFN &lt;&lt; SHIFT) | offset<br>"	chapter18 OSTEP
How does the page-table size change as the address space grows? As the page size grows? (	As the address space grows, the page table size also grows (because there will be more bits in the VPN required to store more PTE's)<br>As the page size grows, the page table size gets smaller, because there are less bits available to store the PTE's.	chapter18 OSTEP
What are 5 type of bits stored in a PTE, and what are they used for? (18.3)	"valid bit: whether the page has been allocated by the process<br>protection bits: whether the page can be read, written, and/or executed from<br>present bit: indicates whether this page is physical memory or on disk (ie: has been swapped out)<br>dirty bit: indicates whether the page has been modified since it was brought into physical memory<br>reference bit (aka access bit, use bit): sometimes used to indicate whether a page has been accessed, and useful in determining which pages are popular and should be kept in memory; critical during page replacement for the LRU/CLOCK page replacement policy<br><br>Note that accessing a page not allowed by the ""valid"" and ""present"" bits will generate a trap to the OS<br>"	chapter18 OSTEP
What is a TLB? (19.1)	Translation lookaside buffer, part of the chip's Memory Managmeent Unit (MMU) and is simply a hardware cache of virtual-to-physical address translations.<br>Because of their perf impact, TLBs in a real sense make virtual memory possible.<br>	chapter19 OSTEP
When context-switching between processes, the translations in the TLB for the last process are not meaningful for the about-to-be run process. How does the hardware/OS handle this? (19.5)	We can either:<div>1. Flush the TLB on context switches, thus emptying it before running the next process OR<br>2. Provide an address space identifier (ASID) field in the TLB. Basically is a PID but with fewer bits, allowing TLB to store translations from multiple processes.<br></div>	chapter19 OSTEP
What is a PTE valid bit, and how does it diff from a TLB valid bit? (19.5)	When a PTE is marked invalid, it means that the page has not been allocated by the process, and should not be accessed by the program.<br>A TLB valid bit refers to whether a TLB entry has a valid translation within it. This is useful when booting, or performing context switches, to ensure the about-to-run process does not accidentally use the virtual-to-physical translation from a previous process.<br>	chapter19 OSTEP
When is a random strategy better than LRU for cache replacement? (19.6)	When a program loops over N + 1 pages with a TLB of size N; in this case, LRU missus upon every access, whereas random does much better.	chapter19 OSTEP
With a 32 bit address space, 4 KB pages, and 4 byte PTE, how many VPN's will we have? How big is the resulting page table? (20.1)	Number of VPN's = (address space bits) - (page offset bits) = 32 bits - 12 bits = 20 bits<br>Page table size = (size of page) * (number of pages) = 4 byte * 2^20 = 2^22 ~= 4 MB<br>	chapter20 OSTEP
What is the crux between large page tables vs larger pages? (20.1)	With larger page tables, requiring one page table per process, we'd end up allocating lots of memory just for page tables.<br>With larger pages, our page tables get smaller, but increasing our internal fragmentation.<br>Solutions:<br>1. hybridizing segmentation into our page table<br>2. multi-level page table<br>3. inverted page tables (1 page table for all processes)<br>4. swapping page tables to memory<br>	chapter20 OSTEP
What is a multi-level page table? What problem does it solve? (20.3)	Solves the problem of linear page tables taking too much memory, by turning it into something like a tree.<br>Chops up the page table into page-sized units (Page Directory Entries); then if an entire page of PTE's is invalid, don't allocate the page of the page table at all.<br>To track whether a page of the page table is valid (and if valid, where it is in memory), use a new structure called a Page Directory. This is used to tell where a page of the page table is, or that the entire page of the page table contains no valid pages.<br><br>	chapter20 OSTEP
What is the crux solved by swap space? How is swap space viewed by the process?	Going beyond physical memory. Let's the OS make use of a larger (and thus slower) device to transparently provide the illusion of a larger virtual address space.<br><br>As far as the process is concerned, it has its own private, contiguous virtual memory. Behind the scenes, pages are placed in arbitrary (non-contiguous) locations in physical memory, and sometimes not even present in phys memory, requiring a fetch from disk.<br>	chapter21 OSTEP
"What is the ""present bit""?"	When the hardware looks in the PTE, it may find that the page is not present in the physical memory. The way the hardware (or the OS, in a software-managed TLB approach) determines this is through a present bit in each Page Table Entry.<br>	chapter21 OSTEP
What is a page fault? When does it occur? How is it related to a syscall? (21.3)	"Signals the kernel to go set up the virtual-to-physical page table mapping. Kernel is notified via a trap, so it's similar to a syscall but happens implicitly.<br><br>Occurs during a TLB miss where the page is not present in physical memory. Handled by the OS page-fault handler, which arranges for the transfer of the desired page from disk to memory, perhaps first replacing some pages in memory to make room for those soon to be swapped in.<br><br><div>aka ""page miss"", but usually called a ""fault"" because the hardware doesn't know what to do, thus transfers control to the OS. This is identical to what happens when a process does something illegal.<br></div>"	chapter21 OSTEP
How does the OS decide when to replace (evict) a page from physical memory into swap space? How does it do this efficiently? (21.6)	Using high watermark (HW) and low watermark (LW).<br>When the OS notices that there are fewer than LW pages available, a background thread that is responsible for freeing memory runs. The thread evicts pages until there are HW pages available.<br><br>The background thread, sometimes called the swap daemon or page daemon, then goes to sleep, happy that it has freed some memory for running processes and the OS to use.<br><br>This allows the system to cluster/group page writes, increasing the efficiency of the disk.<br>	chapter21 OSTEP
What is the formula for measuring average memory access time (AMAT) when a TLB looks up the PTE? (22.1)	AMAT = Tm + (Pmiss * Td)<br>Tm = cost of accessing memory<br>Td = cost of accessing disk<br>Pmiss = probability of not finding the data in a cache (between 0-1.0)<br>	chapter22 OSTEP
What are the 3 types of cache misses in computer architecture? (the 3 C's) (22.2)	compulsory miss (aka cold-start miss) - when the cache is empty to begin with and this is the first reference to the item<br>capacity miss - occurs because the cache ran out of space and had to evict an item to bring a new item into the cache<br>conflict miss - arises in hardware because the limit on where an item can be placed in a hardware cache, due to set-associativity. Does not arise in OS page cache because such caches are fully-associative (ie: there are no restrictions on where in memory a page can be placed)<br>	chapter22 OSTEP
What is the Optimal Replacement Policy, wrt measuring page caching policies? Why is it useful? (22.2)	This policy results in the fewest number of misses overall. Evicts the page that will be accessed furthest in the future from the cache. Assumes that we know how each page is used in the future.<br><br>Useful in benchmarking, as comparing against optimal is useful (eg: a 80% hit rate for a policy isn't meaningful in isolation, but if we know that optimal is 82%, then we can say it's quite close to optimal).<br>	chapter22 OSTEP
What is the principle of locality? And 2 types of this? (22.4)	Basically just an observation about programs and their behavior.<br>Programs tend to access certain code sequences (eg: in a loop) and data structures (eg: an array accessed by the loop) quite frequently; we should thus try to use history to figure out which pages are important, and keep those pages in memory when it comes to eviction time.<br><br>2 examples are temporal and spatial locality.<br>	chapter22 OSTEP
What are 3 kinds of page replacement policies? How do they compare against a random workload? A looping workload? On a realistic workload? (22.6)	"FIFO, Random, and LRU.<br>They are all tied against a random workload, on average, although the Random policy is non-deterministic.<br><br>LRU is generally best on a realistic workload, because it can take advantage of temporal locality.<br>LRU and FIFO do poorly on the ""looping workload"" edge case (eg: looping sequential workload of 50 pages on a cache of size 49, will be all misses).<br>Random does best on a looping workload, since it avoids this edge case.<br><br>LRU is also hard to implement without degrading performance caused by scanning through all page timestamps.<br>"	chapter22 OSTEP
What other policy must the VM subsystem employ, in addition to page replacement/eviction policy? And a couple of options? (22.10)	"The OS also has to decide <b>when</b> to bring a page into memory. Aka ""page selection"". Some options:<br><b>demand paging</b>&nbsp;- to bring the page into memory when it is accessed<br><b>prefetching</b>&nbsp;- should only be done when there is a reasonable chance of success. Eg: assume that if a code page P is brought into memory, that code page P+1 will likely soon be accessed and thus brought into memory too.<br>"	chapter22 OSTEP
What is thrashing? 3 ways to deal with it? (22.11)	When the memory demands of a set of running processes exceeds the available physical memory. Results in the system to be constantly paging. Solutions:<br><b>admission control -</b>&nbsp;when thrashing, OS decides not to run a subset of processes<br><b>OOM Killer -</b> daemon chooses a memory intensive process and kills it. This can have problems, however, if the process is, say, your X server.<br><br>Because paging to disk is so expensive, the cost of frequent paging is prohibitive. Thus, the best solution to excessive paging is often a simple (if intellectually unsatisfying) one: buy more memory.<br>	chapter22 OSTEP
Why does null pointer access cause seg faults? (Walk through the lookups)	Because that segment, on page 0, is saved for providing null pointer access.<br><br>eg:<br>int *p = NULL; // set p = 0<br>*p = 10;<br><br>Hardware tries to look up VPN (also 0 here) in the TLB, and suffers a TLB miss.<br>Page table is consulted for entry VPN 0, which is found to be marked invalid.&nbsp;<br>	chapter23 OSTEP
What is the copy-on-write (COW) wrt VMS optimization? An example use case?	If a resource is duplicated but not modified, it is not necessary to create a new resource; the resource can be shared between the copy and the original. Modifications must still create a copy, hence the technique: the copy operation is deferred until the first write. By sharing resources in this way, it is possible to significantly reduce the resource consumption of unmodified copies, while adding a small overhead to resource-modifying operations.<br><br>Wrt VMS, when the OS needs to copy a page from one address space to another, instead of copying it, it can map it into the target address space and mark it read-only in both address spaces. If both address spaces only read the page, no further action is taken, and thus the OS has realized a fast copy without actually moving any data.<br>If one of the address spaces does indeed try to write to the page, it will trap into the OS. The OS will then notice that it's a COW page, and thus (lazily) allocate a new page, fill it with the data, and map this new page into the address space of the faulting process. The process then continues with its own private copy of the page.<br><br>Eg: Implementation of fork syscall. Typically, the process does not modify any memory and immediately executes a new process, replacing the address space entirely. Thus, it would be wasteful to copy all of the process's memory during a fork, and instead the copy-on-write technique is used.<br><br>https://en.m.wikipedia.org/wiki/Copy-on-write<br>	chapter23 OSTEP
What are “huge pages” in Linux? Pros and cons? (23.3)	Huge pages support 2MB up to 1GB pages.<br>Pros: Improves perf on “big memory” workloads by allowing a process to access a large tract of memory without TLB misses by using fewer slots in the TLB.<br>Cons: Higher internal fragmentation. Swapping also does not work well with huge pages.<br>The 4KB page size is not the universal solution it once was; growing memory sizes demand that we consider larger pages as part of a necessary evolution of VM systems. Linux’s slow adoption of this hardware-based technology is evidence of this change.<br>	chapter23 OSTEP
<div>What are 2 major differences between threads and processes, wrt context switching? (26.1)</div>	<div>Switching between threads and processes is mostly the same (switching program counter, registers, etc), except:</div><div>The address space remains the same (there is no need to switch which page table we’re using)</div><div>The thread state is stored in a thread control block (TCB) instead of a process control block (PCB). This includes the thread’s stack (aka “thread local storage”)</div>	chapter26 OSTEP
Why use threads? (2 reasons) 26.1	<div>parallelism - splitting up a task across multiple threads, especially when sharing data (eg: updating a large array)</div><div>overlap - enabling overlap of I/O/ etc, within a single process. Using threads is a natural way to utilize the CPU and get unstuck.</div><br><div>Threads share an address space, making it easy to share data. Processes are a more sound choice for logically separate tasks where little sharing of data structures in memory is needed.</div>	chapter26 OSTEP
<div>What is a critical section? (26.7)</div>	<div>A piece of code that accesses a shared resource, usually a variable or data structure.</div>	chapter26 OSTEP
What are mutual exclusion primitives? (26.7)	<div>It's a way to avoid bugs with race conditions and critical sections by guaranteeing that only a single thread ever enters a critical section, thus avoiding races, and resulting in deterministic program outputs.</div>	chapter26 OSTEP
<div>What is a race condition (or more specifically, a data race)? How does it relate to an indeterminate program? (26.7)</div>	<div>The results depend on the timing execution of the code. With some bad luck (ie: context switches that occur at untimely points in the execution), we get the wrong result.</div><div>Arises if multiple threads of execution enter the critical section at roughly the same time; both attempt to update the shared data structure, leading to a surprising (and perhaps undesirable) outcome.</div><div>An indeterminate program consists of one or more race conditions.</div>	chapter26 OSTEP
<div>Why do we have to initialize our locks? (with PTHREAD_MUTEX_INITIALIZER or pthread_mutex_init) (27.1)</div>	<div>All locks must be properly initialized in order to guarantee that they have the correct values to begin with and thus work as desired when lock and unlock are called.</div>	chapter27 OSTEP
<div>What are the three criteria for evaluating locks? (28.4)</div>	<div>Mutual exclusion - Does the lock work, preventing multiple threads from entering a critical section?</div><br><br><div>Fairness - Does each thread contending for the lock get a fair shot at acquiring it, once it's free? More extreme case: will any thread starve while contending?</div><br><br><div>Performance - Time overheads when using the lock. Consider single thread, multi-thread, and multi-CPU cases.</div>	chapter28 OSTEP
What is a spin lock? When to use it? (28.6)	<div>Refers to the way a thread waits to acquire a lock that is already held: it endlessly checks the value of a flag.</div><div>The cost is exceptionally high on a uniprocessor, where the thread that the waiter is waiting for cannot even run (at least not until a context switch occurs)!</div><div>Use it when you need something simple, you have multiple processors, and you don't mind the perf hit.</div><div><br></div>	chapter28 OSTEP
<div>What are 3 techniques in developing a lock that doesn’t needlessly waste time spinning on the CPU while other threads are running? (28.12)</div>	"<div>As the scheduler determines which thread to run next, there is potential for waste/starvation as the thread spins. Here are 3 ways to address that:</div><br><br><div>Yield: leverage this syscall to move the caller from running to a ready state, essentially descheduling itself (aka ""cooperative scheduling"")</div><br><div>Using queues and thread sleep: By combining test-and-set with an explicit queue of lock waiters, we can leverage FIFO to avoid starvation. (eg: Solaris uses ""park()"" to put a calling thread to sleep, and ""unpark(threadID)"" to wake the thread by its id.) This also avoids spin-waiting in the user-defined critical section, and only spins while adding a lock to the queue.</div><br><br><div>Two-phase (hybrid) lock: This lock realizes that spinning can be useful, particularly if the lock is about to be released. First phase it spins, hoping to acquire the lock. Second phase puts the caller to sleep, waking when the lock becomes free later. An example of this is the Linux futex lock, which uses an internal queue per futex to track the lock's waiters.</div>"	chapter28 OSTEP
What are 3 hardware primitives that make atomic locking possible? (28.7)<br>	"<div><b>Test-and-set instruction</b> (aka atomic exchange, ""xchg"" on x86):</div><div>pseudo-code representation of the asm:</div><div>(int *old_ptr, int new) =&gt; { int old = *old_ptr; *old_ptr = new; return old; }</div><div><br></div><div>By ""testing"" the old value, while simultaneously ""setting"" the new value, we can build a simple spin lock:</div><br><div>while (TestAndSet(&amp;lock-&gt;flag, 1) == 1); // spin-wait (do nothing)</div><br><div><b>compare-and-swap</b> (aka compare-and-exchange on x86) :</div><div>pseudo-code representation of the asm:</div><div>(int *ptr, int expected, int new) =&gt; { int actual = *ptr; if (actual == expected) { *ptr = new; } return actual; }</div><br><div>A little more powerful than test-and-set, but we can build a spin lock just the same:</div><div>while (CompareAndSwap(&amp;lock-&gt;flag, 0, 1) ==1); // spin</div><br><div><b>fetch-and-add:</b> atomically increments a value while returning the old value at a particular address. This ensures progress for all threads, because the thread gets assigned a ""ticket"" value, and the ""turn"" iterates through the tickets FIFO. But it can cause lots of unnecessary spinning.</div><div>pseudo-code representation of the asm:</div><div>(int *ptr) =&gt; {old = *ptr; *ptr = old + 1; return old;}</div><div>int myturn = FetchAndAdd(&amp;lock-&gt;ticket);</div><div>while (lock-&gt;turn != myturn); // spin</div>"	chapter28 OSTEP
What is Lauer's Law regarding code size? And the aphorism? (28.10)	"<div>Short, concise, code is always preferred; it is likely easier to understand and has fewer bugs. So instead of bragging about how much code we write, we should brag about how little code we write to accomplish some task.</div><div>""If the same people had twice as much time, they could produce as good of a system in half the code"".</div>"	chapter28 OSTEP
<div>What is priority inversion when running multiple threads, and 3 ways to avoid it? (28.14)</div>	"<div>When threads have different priorities, it can cause abnormal delays while waiting on a lock. Can be addressed by (1) avoiding spin locks, or more generally, (2) a higher-pri thread waiting on a lower-pri thread can temporarily boost the lower thread's priority (aka ""priority inheritance). Or (3) ensuring all threads have the same priority.</div><br><div>Eg: Imagine three threads, T1, T2, T3, with T3 at highest-pri, T1 lowest. If T1 grabs the lock. T3 then starts, and because it's higher-pri, runs immediately (preempting T1). T3 tries to acquire the lock that T1 holds, but gets stuck waiting, because T1 still holds it. If T2 starts to run it will have higher priority than T1, and thus it will run. So T3, which is higher pri than T2, is stuck waiting on T1, which may never run now that T2 is running.</div>"	chapter28 OSTEP
"What is ""perfect scaling"", wrt measuring perf on lock-based data structures? (29.1)"	<div>It's an ideal metric, where threads complete just as quickly on multiple processors as a single thread does on one. Even though more work is done, it is done in parallel, and hence the time taken to complete the task is not increased as we add more threads.</div>	chapter29 OSTEP
<div>What is an approximate counter wrt lock-based concurrent data structures? What does it help us achieve? What is the tradeoff? (29.1)</div>	<div>Represents a single logical counter via numerous <i>local</i> physical counters, one per CPU core, as well as a single <i>global</i> counter. Thus it achieves perfect scaling, while sacrificing some accuracy in the count.</div><div><br></div><div>Specifically, on a machine with four CPU's, there are four local counters and one global one. In addition to these counters, there are also locks: one for each local counter, and one for the global counter.</div><div><br></div><div>When a local counter reaches a user-specified threshold (aka approximation factor), the global count can be incremented by its value, and then it'll be set to 0. Thus, a higher threshold has better perf but the global count lags farther, and vice-versa. The lag will be at most the number of CPU's multiplied by the threshold.</div>	chapter29 OSTEP
What is a condition variable? What crux does it solve? (30.1)	<div>Useful for a thread to wait for some condition to become true before proceeding. Spinning is grossly inefficient.</div><br><div>Can be used to solve the producer/consumer problem (aka bounded buffer problem), or any generic multi-threaded problem where one thread must wait for a condition to be true in another thread before it can proceed.</div>	chapter30 OSTEP
<div>Mesa semantics vs Hoare semantics, wrt the meaning of a signal? What’s the aphorism? (30.2)</div>	<div>Mesa semantics is most common, where signalling a thread only wakes them up;&nbsp; there is no guarantee that when the woken thread runs, the state will _still_ be as desired. (eg: another thread might change the state before the woken thread runs)</div><br><br><div>Hoare semantics is harder to build, but provides a stronger guarantee that the woken thread will run immediately upon being woken.</div><div>Virtually every system ever built employs Mesa semantics.</div><div>Thus we must “Loop before you leap!” to ensure that the state is as desired upon waking, or else the thread should go back to sleep again.</div><br><div>Another reason to loop is because spurious wakeups can occur, depending on the thread package, where two threads can be woken by just a single signal.</div>	chapter30 OSTEP
<div>What is a covering condition? What’s an example where we need it? What are the drawbacks? (30.3)</div>	<div>Instead of waking a single thread (pthread_cond_signal), we wake&nbsp;<i>all</i> threads (pthread_cond_broadcast)</div><div><br></div><div>Memory allocation example:</div><div>Assume 0 bytes free</div><div>Thread 1 waiting on a call to allocate(100)</div><div>Thread 2 waiting on a call to allocate(10)</div><div>Call to deallocate(50)</div><div>&nbsp;</div><div>If we only wake Thread 1, it’ll loop on it’s “while (bytesleft &lt; size)” condition, and go back to sleep. How do we know to wake Thread 2?</div><br><div>By broadcasting, we can wake all threads, including Thread 2. Note that changing signals to broadcasts causes perf issues and is generally a bad idea, but is correct in specific scenarios like this one.</div>	chapter30 OSTEP
<div>What is a semaphore, and how is it used? And 3 salient aspects of its interface? An analogy of its usage? An aphorism for its usage in place of locks and cond vars? (31.1)</div>	"<div>A semaphore is a synchronization primitive, and can be used as a single. primitive for both locks and condition variables.</div><br><br><div>It holds a value representing the number of free resources. Example: Library with 10 study rooms, which the librarian tracks or makes students wait when unavailable.<br></div><br><br><div>3 salient aspects:</div><div>1. The ""sem_wait"" routine will either return right away (because the semaphore value was one or higher when ""sem_wait"" was called), and decrementing the value.<br><br>Or it will cause the caller to suspend execution until awoken and set to a positive value.</div><br><div>2. The ""sem_post"" routine does not wait for a condition like ""sem_wait', but increments the value of the semaphore, and if a thread is waiting on it and needs to be awoken, it does so.</div><br><div>3. The value of a semaphore, when negative, is equal to the number of waiting threads. (although this value is generally not seen by users of the semaphore, it's helpful for knowing how it functions). Note that the Linux semaphore impl doesn't exactly follow this, since it's harder to implement.</div><br><div>One could view semaphores as a generalization of locks and condition variables; however, is such a generalization needed? And, given the difficulty of realizing a condition variable on top of a semaphore, perhaps this generalization is not as general as you might think. (eg: Windows built condition vars on top of semaphores, and many tricky bugs ensued)</div><div><br></div><div><div>TLDR: be careful when generalizing: ""don't generalize, generalizations are generally wrong"" (31.7)</div></div>"	chapter31 OSTEP
<div>What are 2 types of semaphore use cases, and what are the initialization values for each use case? What's the initialization values when using semaphores for a bounded buffer solution? (31.2, 31.3, 31.4)</div>	"<div><b>Binary Semaphore</b> - for implementing a mutex lock. Since locks only have 2 states (held and not held), it's called a ""binary semaphore"". Can be implemented in a much simpler fashion than the generalized semaphore. The initial value is 1 because we're only willing to give away 1 resource after initialization. Once the lock is locked, we'll have 0 left to give.</div><br><div><b>Ordering Semaphore</b> - for implementing a condition var. One thread is _waiting_ for something to happen, another thread is making that something happen and then _signaling_ that it happened. The initial value is 0 because there is nothing to give away at the start; only when the child is done is the resource created, at which point, the value is incremented to 1.</div><br><div>For bounded buffers (producer/consumer), we set an ""empty"" semaphore to be the length of the bounded buffer, and a ""full"" semaphore to 0. Upon waiting on ""empty"" decrements, we'll increment/signal the ""full"" semaphore, and vice-versa, to ensure that adding/removing an item to/from the buffer is balanced. (additionally, note that we'll still need a binary semaphore (or mutex) to ensure that the updates in the critical section are atomic)</div>"	chapter31 OSTEP
<div>What is a reader writer lock, and what's its purpose? What locking pattern does it use? What is the caveat (aphorism) when implementing one? (31.5)</div>	"<div>Reader/writer is a special type of lock that allows for many lookups (reads) to proceed concurrently as long as we can guarantee that no insert is on-going. This represents a more flexible locking primitive that admits that different data structure accesses might require different kinds of locking.</div><br><br><div>Patterns similar to this reader code are common: the first thread into a section locks a semaphore (or queues) and the last one out unlocks it. In fact, it is so common we could give it a name and wrap it up in an object. The name of the pattern is `Lightswitch`, by analogy with the pattern where the first person into a room turns on the light (locks the mutex) and the last one out turns it off (unlocks the mutex). - Little Book of Semaphores, ch 4.2</div><br><div>Reader/writer can be implemented fairly simply with semaphores, although they often add more overhead (especially with more sophisticated implementations), thus do not end up speeding up performance as compared to just using simple and fast locking primitives. Having to handle starvation is one example.</div><br><br><div>Although a reader/writer lock sounds cool, they are complex, and complex means slow. Sometimes a big ole lock works best because they are simple to implement and fast. So try the simple and dumb approach first, becasue sometimes ""big and dumb is better"".</div>"	chapter31 OSTEP
<div>2 kinds of non-deadlock concurrency bugs? How can we fix them? (32.2)</div>	"<div><b>Atomicity Violation</b> - The desired serializability among multiple memory accesses is violated (ie: a code region is intended to be atomic, but the atomicity is not enforced during execution). Note: code region is also a ""critical section"". Mutex should fix this.</div><br><div><b>Order Violation</b> - The desired order between two (groups of) memory accesses is flipped (ie A should always be executed before B, but the order is not enforced during execution). Condition variables are an easy and robust way to add this style of synchronization.</div>"	chapter32 OSTEP
<div>How does some types of encapsulation almost incite us to deadlock? What's an example? (32.3)</div>	<div>As software developers, we are taught to hide details of implementations and thus make software easier to build in a modular way. Unfortunately, such modularity does not mesh well with locking.</div><br><div>eg:</div><div>Vector v1, v2; v1.add_all(v2);</div><br><div>If there is no total ordering in the way the locks for v1 and v2 are acquired, then calling v2.add_all(v1) at nearly the same time can result in deadlock, all in a way that is quite hidden from the calling application.</div>	chapter32 OSTEP
<div>What are the 4 conditions for deadlock? (32.3)</div>	<div><b>Mutual Exclusion</b> - Threads claim exclusive control of resources thatthey require (eg: a thread grabs a lock)</div><div><br></div><div><b>Hold-and-wait</b> - Threads hold resources allocated to them (eg: locks they have already acquired) while requesting additional resources (eg: more locks)</div><div><br></div><div><b>No Preemption</b> - Resources (eg: locks) cannot be forcibly removed from threads holding them</div><div><br></div><div><b>Circular Wait</b> - There exists a circular chain of threads such that each thread holds one or more resources (eg: locks) that are being requested by the next thread in the chain</div>	chapter32 OSTEP
5 ways for preventing deadlock problems? Downsides for each approach? (32.3)	"<div><b>Circular wait</b> - leverage total ordering (eg: always acquire L1 before L2) or partial ordering (eg: L1 before L2, and L2 before L3 beofre L4) in how locks are acquired. Con: It's just a convention, and easy to ignore the locking protocol to cause deadlock</div><br><div><b>Hold-and-wait</b> - Acquire all locks at once, atomically (eg: a global lock to begin/end lock acquisition). Cons: prevents encapsulation (eg: when calling a routine, we have to know exactly which locks are held to acquire them ahead of time), decreases concurrency.</div><br><div><b>No Premption</b> - leverage a ""try_lock"" to back out of lock ownership, and perhaps re-try lock acquisition later. Note that this doesn't _add_ preemption (forcibly taking away a lock from a thread that owns it), but preempts a thread from its own ownership in a graceful way. Cons: adds complexity as we have to give up our resources (eg: free memory) when backing out. Also potential for livelock.</div><br><div><b>Mutual Exclusion</b> - Avoid the need for a lock at all by using a lock-free approach, via powerful hardware instructions. Eg: using compare-and-swap assembly instruction instead of acquiring a lock, doing an update, and releasing it. Con: requires hardware support</div><br><div><b>Scheduling</b> - Requires knowledge about which locks various threads might grab during execution, and schedule them in a way that no deadlock can occur. Con: Not widely used. Usually limited to embedded systems, where we have knowledge of all tasks and the locks they need. Also has perf issues when multiple threads are scheduled to run serially due to overlapping lock usage.</div>"	chapter32 OSTEP
What is livelock? How can we address it? (32.3)	"<div>Two or more threads attempting a sequence (eg: ""try_lock"") and repeatedly failing to acquire both locks. The sequence runs over and over (so not deadlock), but no progress is being made.</div><br><div>One solution is to add a random delay before looping back and trying it all over again, decreasing the odds of repeated interference among competing threads.</div>"	chapter32 OSTEP
<div>What’s the difference between blocking (synch) vs non-blocking (async) interfaces? What is an example where non-blocking interfaces are essential? 33.2</div>	<div>Blocking interfaces do all of their work before returning to the caller; non-blocking interfaces begin some work but return immediately; thus letting whatever work that needs to be done get done in the background.</div><div>The usual culprit of blocking calls is I/O of some kind (eg: reading to disk, and waiting for that request to return)</div><div>Non-blocking interfaces can be used in any style of programming (eg: with threads), but are essential in the event-based approach (eg: an event loop), as a call that blocks will halt all progress.</div>	chapter33 OSTEP
<div>What happens when we block on an event-based server? (33.5)</div>	<div>With an event-based approach, there are no other threads to run; just run the main event loop. And this implies that if an event handler issues a call that blocks, the _entire_ server will do just that: block until the call completes. When the event loop blocks, the system sits idle, so blocking calls should never be allowed.</div><div>Event-based servers enable fine-grained control over scheduling of tasks. However, to maintain such control, no call that blocks the execution of the caller can ever be made; failing to obey this design tip will result in a blocked event-based server.</div>	chapter33 OSTEP
What are unix signals, how are they triggered, and 3 examples? (33.6)	"<div>Signals provide a way to communicate with a process. Specifically, a signal can be delivered to an application; doing so stops the application from whatever it is doing to run a signal handler (ie: handler can be some code in the app, or some default behavior).</div><br><br><div>Users can send signals via the kill command (yes, this is an old and aggressive name) - ""kill -HUP 36705""<br><br>Sometimes the kernel itself does the signaling, like when the program encounters a segmentation violation, the OS sends a SIGSEGV. Unless the program catches it, the default behavior is to kill the signal.</div><br><br><div>Examples: <br><br>HUP (hang up, indicating that the controlling pseudo or virtual terminal is closed), <br><br>INT (interrupt), <br><br>SEGV - This signal is sent to process when it makes an invalid memory reference, or segmentation fault (ie makes a seg violation)<br><br>SIGSTOP - The STOP signal instructs the operating system to stop a process for later resumption. Cannot be ignored or handled by process itself.<br><br>SIGKILL - Forcefully terminate a process. With STOP, this is one of two signals which cannot be intercepted, ignored, or handled by the process itself.<br></div>"	chapter33 OSTEP
<div>What are the 3 layers of the memory/io bus hierarchy? Why do we need a hierarchical structure like this? (36.1)</div>	<div>Memory Bus (or interconnect) - attaches the cpu to the main memory of the system</div><div>I/O bus - General connect for some devices (eg: PCI, even graphics or other higher-performance I/O devices)</div><div>Peripheral bus - Connect slow devices to the system (eg: SCSI, SATA, USB, disks, mice, keyboards)</div><br><div>Simply put, physics and cost require this hierarchical structure. The faster a bus is, the shorter it must be; thus, a high-performance memory bus does not have much room to plug devices and such into it. In addition, engineering a bus for high perf is costly. The benefits of placing the slower devices on the larger peripheral bus is manifold: we can fit more of them there too.</div>	chapter36 OSTEP
<div>What is firmware? (36.2)</div>	<div>Software running within a hardware device.</div>	chapter36 OSTEP
<div>What are 3 techniques used by a FS to address the problem of waiting on IO to/from disk? What's the tradeoff between them? (36.4)</div>	<div>Interrupt service routine (ISR) (aka interrupt handler) - allows for overlap of a of computation and I/O: OS issues a request, puts calling process to sleep, and context switch to another task. Devices raises a hw interrupt when finally finished w the operation to wake the waiting process.</div><br><div>Use polling (better on a fast device) - using an interrupt in this case might actually slow down the system: switching to another process, handling the interrupt, and switching back to the issuing process is expenisve. It’s better to poll in this case.</div><br><div>coalescing interrupts - when a device needs to raise an interrupt, it first waits for a bit before delivering the interrup to the CPU. While waiting, other requests may come in, and thus be coalesced into a single interrupt.</div>	chapter36 OSTEP
<div>What are 2 methods of disk device interaction from a process? (36.6)</div>	<div>privileged instructions - the OS controls the devices, and is thus the only entity allowed to directly communicate with them (eg: “in” and “out” instructions on x86). Prevents just any program read/writing to disk.</div><br><div>mmapped I/O - The hardware makes device registers available as if they were memory locations. To access a particular register, the OS issues a load (to read) or store (to write) the address; the hardware then routes the load/store to the device instead of main memory.</div><br><div>No great advantage of one over the other. Both methods are in widespread use today.</div>	chapter36 OSTEP
<div>How is the OS device-neutral, hiding details of device interactions from major OS subsystems? 2 down sides? (aphorism for this?) (36.7)</div>	"<div>The OS leverages abstractions, with device drivers as the lowest level. Filesystem and lower level storage mgmt apps talk only through a generic block interface, which interfaces with the drivers.</div><br><div>Downsides:</div><div>1.The generic interface can cause us to miss out on some features that devices offer. Like how SCSI devices have rich error reporting, while others only have generic IO error codes, causing the file system to lose out on the extra details SCSI provides.</div><br><div>2. Device drivers take a lot of code, since they are needed for any device you might plug into your system. Over 70% of Linux kernel code is found in device drivers (high for Windows too!)</div><div>""when ppl tell you that the OS has millions of lines of code, what they are really saying is that the OS has millions of lines of device-driver code”</div>"	chapter36 OSTEP
<div>What is the address space of a drive? What is a block, sector, seek, and track on a disk drive?(37.1, 37.2)</div>	<div>Address space of the drive is an array of sectors numbered from 0 to n-1 on a disk with n sectors.</div><div>A sector is a 512-byte. Manufacturers can only guarantee that a single 512-byte write is atomic - if an untimely power loss occurs, only a portion of a larger write may complete (aka “torn write”).</div><div>Block is a group of sectors the OS can point to (allowing it to work with larger drives). Typically 4KB in size.</div><div>Track is a concentric circle of sectors.</div>	chapter37 OSTEP
<div>On writing to disk drives, what is write back caching vs write through caching? Why is this important? (37.3)</div>	<div>Should the disk acknowledge that the write has completed when it has put the data in its memory, or after the write has actually been written to disk? At what point do we delete the write from the disk's RAM cache?</div><br><div>Write back (aka immediate reporting) - is the former, and can make the drive appear “faster”, but can be dangerous if ordering is important (eg: for journaling)<br><br>It's important when ordering writes to disk.</div>	chapter37 OSTEP
<div>How are transfer rates calculated for a disk drive? How does it affect 2 kinds of drives we buy? What’s the most important design tip when improving transfer rates? (37.4)</div>	"<div>Tio = Tseek +Trot + Ttransfer</div><br><div>Trate = Tsize / Tio</div><br><div>high end ""perf"" drives - when optimizing for faster transfer rate</div><div>low end ""capacity"" drives - when optimizing for cost per byte</div><br><div>But having a sequential vs random workload is the most significant factor in transfer rates, resulting in a factor of 200-300x. While transfer rates for each workload across high vs low end drives is only 2-3x.</div>"	chapter37 OSTEP
<div>What are the 3 advantages of RAID, and why was its deployment so easy? (38.0)</div>	<div>Redundant Arrays of Inexpensive Disks (RAID)</div><div>performance</div><div>capacity</div><div>reliability</div><br><div>RAID provides these advantages transparently to systems that use them, enabling one to replace a disk with a RAID and not change any software.</div>	chapter38 OSTEP
<div>What are 2 types of disk IO workloads, and a use case for each? (38.4)</div>	<div>sequential - searching for string in a file</div><div>random - database mgmt systems</div>	chapter38 OSTEP
<div>What are 4 kinds of RAID? How do their capacity, failure tolerances, and perfs compare? (38.4, 38.5, 38.6, 38.7)</div>	"<div>RAID-0 - Not really a RAID level because there is no redundancy. Will ""strip"" blocks across the disks of the system in a RR fashion, extracting the most parallelism. We can place multiple blocks on each disk before moving to the next disk (aka ""chunk size"").</div><div>capacity: N*B</div><div>tolerates no failures</div><br><div>RAID-1 - mirrors each disk X number of times. Is simple, reliable, and provides good perf at a high capacity cost.</div><div>capacity: N*B/X where X is mirroring level</div><div>Tolerates up to N/2 failures, but in practice, this is only 1 failure</div><br><div>RAID-4 - uses 1 disk for parity information for every group of disks it's protecting. The parity info is calculated from XOR'ing the other disks for that bit, and If we lose a disk, we're able to calculate its value by XOR'ing the existing disks plus the parity disk.</div><div>capacity: (N-1)*B</div><div>tolerates only 1 failure</div><div>Read perf: N-1 * S</div><div>Random write perf: R/2</div><br><div>RAID-5 - almost identical to RAID-4, but the parity blocks are rotated across the other drives. This removes the parity-disk from being the bottleneck as in RAID-4.</div><div>Same capacity and failure tolerances as RAID-4.</div><div>Random write perf: (n/4) * R (factor of 4 for the 2 read and 2 write IO operations)</div>"	chapter38 OSTEP
"<div>What is the ""consistent update problem"" for RAID systems, and how does a write-ahead log solve it? (38.5)</div>"	<div>Occurs when a RAID fails while updating multiple disks during a single logical operation (eg: an untimely power loss). This results in the two copies of a block being inconsistent.</div><br><div>To make this atomic, we record what the RAID is about to do, usually in non-volatile battery-backed RAM. Then in the presence of a crash, we run a recovery procedure to replay all pending transactions to the RAID.</div>	chapter38 OSTEP
<div>What is a file descriptor, its usage, and how are they managed by the OS? How do they relate to entries in the Open File Table? (39.3, 39.5)</div>	"<div>A fd is just an int used to access files. It's used as a capabilty, providing an opaque handle giving you the power to open, then read/write to files (assuming you have the permissions to do so).</div><br><div>Each process maintains an array of file descriptors, each of which refers to an entry in the system-wide Open File Table.</div><br><div>Opening a new file descriptor maps it to a new file table entry in the OFT. But forking a process with a file descriptor will have both processes reference the same entry. Also calling ""int fd2 = dup(fd)"" will create a duplicate fd reference. So the fd: OFT entry relationship is many-to-one.</div><br><div>Only when all processes referencing the fd are done will the file table entry be removed.</div>"	chapter39 OSTEP
<div>What is the Open File Table used for? What are 5 attributes that a file table entry stores? (39.5)</div>	<div>OFT is used for mapping the private, per-process fd's to an OFT entry. Each entry therein tracks which file this access refers to, along with other information:</div><br><div>reference count</div><div>readable</div><div>writable</div><div>pointer to inode reference (this is the file information)</div><div>offset</div>	chapter39 OSTEP
<div>How are offsets managed when reading from a file descriptor? What are the 2 ways an offset can be updated? (39.5, 39.6)</div>	"<div>For each file a process opens, the OS tracks a ""current"" offset in the OFT, which determines where the next read or write will begin reading from or writing to within the file.</div><br><div>Offsets can be updated in 2 ways:</div><div>1. Whenever a read/write takes place, the offset is implicitly updated by the number of bytes read/written.</div><div>2. Setting offset explicitly with lseek command</div>"	chapter39 OSTEP
<div>What is a file? Why is it important for IO? (39.18, CSAPP 1.7.4)</div>	<div>A file is a sequence of bytes - nothing more, nothing less.<br><br>It can be created, read, written, and deleted. It has a low-level name (ie: a number), often called an i-number<br><br>Every IO device is modelled as a file, add all IO in the system is performed by reading and writing files, using a small set of syscalls known as Unix IO<br><br>Very powerful because it provides apps w a uniform view of all varied IO devices that might be contained within the system.<br></div>	chapter39 OSTEP
<div>What is a directory? What does it store? (39.18)</div>	<div>A Directory is a collection of tuples, each containing a human-readable name and a low-level name to which it maps. Each entry refers to either another regular file or a dir.</div><div>Each dir also has a low-level (i-number) itself.</div>	chapter39 OSTEP
<div>What does the lseek syscall do? When is it used? (39.5)</div>	"<div>Changes a variable in OS memory that tracks, for a particular process, at which offset its next read or write will start.</div><br><div>Use it when you want to read a file in any manner other than a sequential start-to-finish (eg: tailing the last n lines of a file, you'll need to `lseek(fd, -1, SEEK_END)` then backtrack n lines)</div><br><div><a href=""https://man7.org/linux/man-pages/man2/lseek.2.html""><span style=""color: rgb(17, 85, 204);"">https://man7.org/linux/man-pages/man2/lseek.2.html</span></a></div>"	chapter39 OSTEP
<div>What are the 3 kinds of files? (along with 5 subtypes in the 3rd category) (39.18)</div>	<div>Regular files - readable/binary/image/compressed etc</div><br><div>Directory files - contains other files</div><br><div>Special files -</div><div>Block - hardware files, mostly present in /dev (eg: sda, sdb)</div><div>Character device - provides a serial stream of i/o (eg: a terminal tty0, tty1)</div><div>(Named) Pipe - streams bytes via FIFO</div><div>Symlink - links to other dir/regular files, sharing the same inode</div><div>Socket -pass info between apps for communication purposes</div>	chapter39 OSTEP
"<div>When we remove a file, why does the OS call ""unlink()""? (39.14)</div>"	"<div>When a file system unlinks a file, it checks a reference count (aka ""link count"") within the inode number. This reference count allows the fs to track how many different file names have been linked to this particular node.</div><br><div>When unlink() is called, it removes the ""link"" (the dirent tuple) between the human-readable name (the file that is being deleted) to the given inode number, and decrements the reference count. Only when the reference count reaches zero does the fs also free the inode and related data blocks, and thus truly ""delete"" the file.</div><br><div>You can see the reference count of a file using stat() (eg: ""stat myfile.txt"")</div>"	chapter39 OSTEP
"<div>What are 3 tradeoffs between hard and symbolic (aka ""soft"") links? (39.14, 39.15)</div>"	<div>Both allow for accessing the original file through the link.</div><br><div>3 tradeoffs:</div><div>1. Symlinks can be used across volumes - Hard links create a reference (dirent) to the inode, incrementing the file's reference count. Symbolic links allocate a new inode, thus creating a new file, which holds the pathname of the linked-to file.</div><br><div>2. Symlinks can point to dirs - hard links can't for fear of introducing loops in the fs hierarchy</div><br><div>3. Symlinks can end up as dangling references - Removing the original file causes a symlink to point to a pathname that no longer exists. Doing the same on a hard link only decrements the inode's reference count - the hard link still works as a file name pointer.</div>	chapter39 OSTEP
<div>What are 3 syscalls made when writing to a file? (39)</div>	"<div>fd=open(""/z/x"", O_WRONLY|O_APPEND);</div><div>write(fd, buf, BLOCKSIZE);</div><div>close(fd);</div>"	chapter39 OSTEP
<div>What are the 5 basic data regions that organize the Fast File System on disk, and how is it divided? (40.2)</div>	"<div>fs is divided into fixed-size blocks, commonly of 4KB in size. Simple fs's use just one block size.</div><br><div>5 parts:</div><div>1. superblock - contains info about how many inodes and data blocks are in the fs, where the inode table and other regions begin, etc. Also likely includes a magic number to identify the file system type.</div><br><div>2 and 3. data and inode bitmaps - (could also be btree or LL) - indicate whether the corresponding object/block is free or in-use</div><br><div>4. inodes - short for ""index node"", arranged in an array and indexed into when accessed. Each inode is ~256 bytes, with metadata like file type, size, protection info, access times. Also defines where its data blocks are, using (in)direct pointers.</div><br><div>5. data region - largest part of the file system, consisting of blocks pointed to by the inodes.</div>"	chapter40 OSTEP
<div>Since a disk is not addressable by byte, how do we fetch data from the disk given the byte number? (40.3)</div>	<div>We need to find the number of blocks mapped to the requested region, then map it to the sectors that we want to request.</div>	chapter40 OSTEP
<div>If each inode only stores a fixed number of pointers to the data blocks, how do we store more data in the inode? (40.3)</div>	<div>Multi-level indexed file systems allow us to leverage indirect pointers - instead of pointing to a block that contains user data, it points to a block that contains more pointers, each of which point to user data.</div><br><div>If a file has 12 direct pointers and 1 indirect pointer, block size of 4KB and 4-byte disk addresses, that adds 1024 pointers per block. So the file can grow to be (12 + 1024) * 4KB or 4144KB.</div>	chapter40 OSTEP
<div>What is the extent approach in a filesystem? What are 2 tradeoffs they have with pointers? (40.3)</div>	<div>An extent is a pointer plus a length in blocks.</div><br><div>Tradeoffs:</div><div>Pointers are more flexible (extents only work when there is enough free space on disk, and files can be laid out contiguously)</div><div>Extents are more compact (pointers use a large amount of metadata per file, esp for large files)</div>	chapter40 OSTEP
<div>What are 3 data structures we can use to manage free space (available data blocks/inodes) in an FFS style file system? Trade offs? (40.4)</div>	<div>btree - Compactly represents which files are free, but more complex. Used by most modern filesystems.</div><div>Bitmap - simple and fast, but uses lots of space. Used in the Unix Fast File System (ext2/3 intellectual predecessor)</div><div>Linked list (aka “free lists”) - used in early file systems, somewhat medium complexity and speed.</div>	chapter40 OSTEP
"<div>What 5 steps does the fs take when issuing open(""/foo/bar/baz"", O_RDONLY) call? What are 2 perf takeaways? (40.6)</div>"	"<div>1. traverse beginning at / dir, calling read (root inode id hard-coded to be 2 in most UNIX fs's),</div><div>2. Once the root  dir's inode is read in, FS looks up the data blocks (containing contents of root dir) to find entry + inode for ""foo"" (dirent data structures)</div><div>3. Recursively traverse pathname, reading in the dir inodes + dirents, until ""baz"" is found</div><div>4. Once baz's inode is found, call open(..) on it</div><div>5. Perform final permissions check, allocate fd for the process in the per-process open-file table, and return it to user</div><br><div>User can now call read() syscall to read from the file, which will be at offset 0 (unless lseek() has been called to explicitly alter the offset)</div><br><br>2 Perf takeaways:<br><div>1. IO generated by the open is proportional to the length of the filename.</div><div>2. Each time an inode's block is read, its last-accessed time field must be updated w a write.</div>"	chapter40 OSTEP
"<div>How many IO's are performed when creating a new file ""bar"" under /foo/bar? And when writing 3 data blocks onto it? (40.6)</div>"	<div>Time increasing downward:</div><br><div>10 IO's for creating /foo/bar:</div><div>read / inode</div><div>read / data</div><div>read /foo inode</div><div>read /foo data</div><div>read + write inode bitmap</div><div>write /foo data updated_at</div><div>read + write /foo/bar inode</div><div>write root data updated_at</div><br><div>Then 5 IO's for each data block n:</div><div>read /foo/bar inode</div><div>write /foo/bar data block n</div><div>read + write data bitmap</div><div>write /foo/bar inode</div><br><div>For a total of 10 + (5 * 3) = 25 IO's</div>	chapter40 OSTEP
<div>What is the durability/performance tradeoff wrt file systems? How do they do it (3 techniques for durability and perf)? What are some use cases for each? (40.7)</div>	<div>Durable: The system must go through the full effort of committing the newly written data to disk, and thus the write is slow (but safe). Use case: database transactions that are adding money to a bank account.</div><div>Done via:</div><div>- fsync()</div><div>- direct I/O interfaces</div><div>- using the *raw disk* interface that avoids the OS altogether</div><br><div>Performance: The system can buffer writes in memory for some time and write them later to the disk (in the background). These writes appear quicker (thus better perceived perf), but it's more complex, and writes not yet committed to disk will be lost. Use case: losing the last few images downloaded by your web browser.</div><div>Done via a (static or dynamically partitioned) cache that:</div><div>- stores reads</div><div>- buffers writes</div><div>- schedules the subsequent I/O's to increase perf</div>	chapter40 OSTEP
<div>Which of the following actions create a data block? create an inode? (40)</div><div>create file</div><div>write file</div><div>create dir</div><div>create hardlink</div><div>unlink</div>	<div>create file - create inode</div><div>write file - create a data block</div><div>create dir - creates a data block and an inode</div><div>create hardlink - no data block nor inode created (unless the dir's data block is full)</div><div>unlink - *might* delete a data block if the file has data. Might also delete the inode if no references are left.</div>	chapter40 OSTEP
"<div>What data structures are updated when a file like /<a href=""http://foo/bar.txt""><span style=""color: rgb(17, 85, 204);"">foo/bar.txt</span></a> is created? Assume it is one block (4KB) long (4 file writes, and 2-4 dir writes) (41.3)</div>"	"<div>4 writes for file:</div><div>allocate new inode</div><div>update inode bitmap</div><div>allocate new data block</div><div>update data bitmap</div><br><div>dir:</div><div>update /foo dir data block w/ new dirent for ""bar.txt""</div><div>(if this doesn't fit in the existing block, will need to allocate a new data block, w associate data bitmap)</div><div>update /foo inode, both to reflect the new length of the dir as well as to update time fields.</div>"	chapter41 OSTEP
"<div>How does FFS organize file system data structures so as to improve performance over Very Simple File System? What are 2 types of allocation policies we need on top of those data structures (making it ""disk aware"")? What's the mantra of these policies? (41.2)</div>"	"<div>FFS: Fast File System, made for Unix and the intellectual predecessor to Linux's ext2/ext3</div><br><div>Uses block groups to organize the drive into consecutive portions of the disk's address space.</div><br><div>Follows the mantra of ""keep related stuff together"" (ie: ""keep unrelated stuff far apart"")</div><br><div>2 policies:</div><div>1. For dirs: Use a block group with a low number of allocated dirs (to balance dirs across groups) and high number of free inodes (has space for files). Can also take into account number of free data blocks.</div><div>2. For files:</div><div>- Allocate data blocks of a file in the same group as its inode, preventing long seeks between inode and data</div><div>- Place all files in the same group as the directory that they are in</div><br><div>This takes advantage of file locality, where files in the same dir are often accessed together. One exception is large files, which can be split across multiple groups as needed.</div>"	chapter41 OSTEP
<div>What is an inode? What are 4 fields saved in an inode? (41)</div>	<div>Inodes store attributes and disk-block locations for an object. May also include metadata.</div><br><div>Some common fields:</div><div>timestamps when it was last modified/accessed</div><div>size in bytes</div><div>link/ref count - how many telling how many hard links point to the inode (or directories it refers to, if this is a directory)</div><div>15 pointers, on modern fs's (12 direct, 1 indirect, 1 doubly indirect, 1 triply indirect)</div>	OSTEP
<div>Name a few invariants that fsck checks (up to 7) (42.2)</div>	"<div>superblock - eg: that the fs size is greater than the number of allocated blocks</div><div><br></div><div>free blocks - to check against allocation bitmaps</div><div><br></div><div>inode state - valid type fields, etc</div><div><br></div><div>inode links - recalcs refs (number of dirs containing a link to each inode)</div><div><br></div><div>duplicates - eg two inodes pointing to the same block</div><div><br></div><div>bad blocks - pointing to something outside its valid range (eg an address points to a block greater than the partition size)</div><div><br></div><div>dir checks - that ""."" and "".."" are the first entries, each inode in dirent is allocated, no dir is linked more than once in the hierarchy</div><div><br></div>"	OSTEP
<div>What is journaling? What's it also known as? What problem does it solve? (42.3)</div>	<div>Journaling (aka write-ahead logging) is probably the most popular solution to the consistent update problem (eg writing to multiple RAID disks in a single operation), and borrowed from the world of DBMS.&nbsp;</div><br><div>Basic idea: before writing to disk, write a little note (somewhere on disk, in a well-known location) describing what you are about to do. Writing this note is the “write ahead” part, and we write it to a structure that we organize as a “log”. In ext3, this journal is stored on disk before the block groups and after the super block</div><br><div>Is made finite w a circular log, which prevents the journal from growing infinitely long.</div>	OSTEP
<div>What precaution must a modern fs take when enforcing ordering between disk writes? 2 ways to overcome it? (42.3)</div>	<div>In olden times, forcing ordering between 2 writes was easy:</div><div>&nbsp;- issue the first write</div><div>&nbsp;- wait for the disk to interrupt the OS when the write is complete</div><div>&nbsp;- issue the second write</div><br><div>With <b>immediate reporting</b>, a disk will inform the OS the write is complete when it simply has been placed in the disk’s memory cache, and has not yet reached the disk.</div><br><div>Can overcome it with:</div><div>&nbsp;- Disabling write buffering</div><div>&nbsp;- Using write barriers, which guarantees that all writes issued before the barrier will reach disk before any writes issued after the barriers</div><br>	OSTEP
<div>What is the Crash Consistency problem for disks, and 2 ways to handle it? What are their tradeoffs? (42.2, 42.3, 42.5)</div>	<div>This is the problem where we must ensure that the fs keeps the on disk image in a reasonable state so that we can recover from crashes (eg: power loss)</div><br><div>1. fsck</div><div>2. Journaling</div><br><div>Tradeoffs:</div><div>&nbsp;- journaling is much faster to recover from crashes (especially on large fses): O(size of disk volume) vs O(size of log)</div><div>&nbsp;- journaling requires some writes to be ordered</div><div>&nbsp;- journaling has some perf overhead (extra writes, garbage collection)</div>	OSTEP
<div>What is the sequence of operations for data journaling (4 things)? For ordered metadata journaling (5 things)? Benefit of metadata journaling? (42.3)</div>	"<div>Typical journaling:</div><div>1. Journal write</div><div>2. Journal commit (write txn commit block to log)</div><div>3. Checkpoint (write contents of update to disk)</div><div>4. Free (mark txn as free in the journal by updating the journal superblock)</div><br><div>Metadata journaling:</div><div>1. Data write (write data to final location)</div><div>2. Journal metadata write (write begin block and metadata to og)</div><div>3. Journal commit</div><div>4. Checkpoint metadata</div><div>5. Free</div><div>Note that 1 isn't required before 2, but both must be written before 3.</div><br><div>By forcing the data write first, a fs can guarantee that a journal’s pointer will never point to garbage. ""Writing the pointed-to object before the object that points to it"" is at the core of crash consistency.</div><br><div>Metadata journaling reduces the amount of traffic to the journal while still preserving reasonable consistency guarantees for both fs metadata and user data.</div><br><div>Some newer techniques involve writing a checksum of the contents in the journal begin and end blocks to avoid writing the TxE block separately.</div><br>"	OSTEP
What is physical vs logical logging, wrt journaling? (42.3)	<div>Physical logging is putting their exact physical contents of the update in the journal.</div><br><div>Logical logging puts a more compact logical representation of the update in the journal (eg: “this update wishes to append data block Db to file X”), which is a little more complex but saves space and improves perf.</div><br>	OSTEP
<div>How are inodes stored in a log-structured file system (LFS)? How do we look them up? (43)</div>	<div>In a typical fs, the inodes are organized in an array and placed on disk at fixed locations. Or in FFS, the inode table is broken into chunks and a chunk is placed in each cylinder group.</div><br><div>In LFS, an *inode map* is used as a level of indirection between inode numbers and the inodes. The imap is just an array, with 4 bytes (a disk pointer) that tells you, for each inode number, its current location on the disk.</div><br><div>The inode map can live on a fixed part of the disk, but since it gets updated frequently, perf would suffer due to disk seeks between each update and the fixed location of the imap.</div><br><div>Instead, LFS places chunks of the inode map right next to where it is writing the new information. Thus when writing a new data block for a file, the new data block, its inode, and a piece of the inode map can be updated all together onto the disk.</div><br><div>Now that pieces of the inode map are spread across disk, we look them up using a <b>checkpoint region (CR)</b>. Stored at fixed locations at the beginning and end of disk, CRs contain pointers to (i.e. addresses of) the latest pieces of the inode map.</div><br><div>The CR is only updated periodically (every 30 sec or so) so perf isn’t ill affected.&nbsp;</div><br>	OSTEP
<div>What is a Log-structured File System (LFS)? What’s the name for the techniques that it uses? What are the 4 motivations for using it? What's the downside? (43.1)</div>	"<div>LFS buffers all updates (including metadata!) in an in-memory segment; when the segment is full, it is written to disk in one long, sequential transfer to an unused part of disk. It never overwrites existing data, but rather always writes segments to free locations, resulting in efficient disk usage. Thus data is written sequentially to a circular buffer called a log.</div><br><div>These techniques, sometimes called ""copy-on-write"", are used by ZFS, btrfs, and the flash translation layer of nearly every SSD.</div><br><div>1. System memories are growing, so we can cache reads better. FS perf is largely determined by its write perf, and LFS optimizes for that.</div><div>2. There is a large gap between random vs sequential I/O perf, and LFS optimizes for the latter.</div><div>3. Existing FS's perform poorly on common workloads - Eg: FFS does many short seeks and rotational delays when writing a file of block size one, even when all the updated blocks are within the same block group (eg: updating bit maps, data blocks, and various inodes)</div><div>4. FS's are not RAID-aware - RAID-4 and RAID-5 have the ""small write problem"" where a logical write to a single block causes 4 physical I/O’s to take place.</div><br><div>Downside: need to reclaim old space; otherwise leaves old copies of data scattered throughout the disk. Cleaning the fs is a source of some controversy here.</div><br><br>https://lwn.net/Articles/353411/"	OSTEP
<div>How can we ensure performant recovery when LFS crashes while writing to disk? How does it work (4 steps)? What is this technique called? (43.12)</div>	<div>Borrowed from the db community, LFS uses a “roll forward” technique to handle crashes without sacrificing perf. This way we don't have to update the checkpoint region (CR) after every update to the log/disk. By using 2 CRs, with timestamps on each, we can:</div><br><div>1. Start at the most recent valid CR (determined by having a txn end block, and/or checksum validation)</div><div>2. Find the end of the log (included within the CR)</div><div>3. Use that to read through the next segments and see if there are any valid updates within it</div><div>4. If so, LFS updates the FS accordingly and recovers much of the data and metadata written since the last checkpoint.</div><br>	OSTEP
<div>How do the parts of a flash-based SSD work together? (3 parts) What are 2 reliability issues with SSD's? (44.1, 44.5)</div>	<div>Flash chips are designed to store one or more bits in a single transistor; the level of charge trapped within the transistor is mapped to a binary value. These are called single/multi/triple-level cells, depending on how many bits are encoded into each cell.</div><br><div>SSD's are composed of:</div><div>banks (aka planes) - a large grouping of cells, coming in 2 sizes: blocks and pages</div><div>blocks - a bank that is typically 128-256 KB in size.</div><div>pages - a bank that is a few KB in size (eg: 4 KB). Before you can write a page in flash, you must delete the entire block.</div><br><div>2 reliability issues with SSD's:</div><div><b>Wear Out</b> - when a flash block is erased and programmed, it accrues a little bit of extra charge. Over time, as that charge builds up, it becomes increasingly difficult to differentiate between a 0 and a 1, until the block becomes unusable.</div><div><b>Program Disturbance</b> - When accessing a particular page within a flash, it's possible for some bits to get flipped in neighboring pages.</div><br>	OSTEP
<div>What is the Flash Translation Layer in an SSD? What are its 2 goals? And 2 cons when addressing those goals (44.5) (note that it's LFS)?</div>	<div>One of the essential functions of this control logic is to satisfy client reads and writes, turning them into internal flash operations as need be.</div><br><div>The FTL takes read and write requests on logical blocks (that comprise the device interface) and turns them into low-level read, erase, and program commands on the underlying physical blocks and physical pages (that comprise the actual flash device). The FTL should accomplish this task with the goal of delivering excellent performance and high reliability.</div><br><div>2 goals:</div><div><b>Increase parallelism</b> - by writing to multiple flash chips in parallel</div><div><b>Reduce write amplification</b> - measured by the total write traffic (in bytes) issued to the flash chips by the FTL divided by the total write traffic (in bytes) issued by the client to the SSD.</div><br><div>Most FTL's are log structured - by optimizing writes in a sequential fashion, we avoid having repeated overwrites to the same cells, and minimize effects of disturbance. FTL also stores an in-memory (SRAM) mapping table of logical block addresses to physical block/page addresses, to enable subsequent reads of block N.</div><br><div>Cons:</div><div>- overwrites of logical blocks lead to garbage (ie: old versions of data around the drive and taking up space)</div><div>- mapping tables are costly, and get larger as the device gets larger</div><br><br><br>https://lwn.net/Articles/353411/	OSTEP
<div>What are 4 disk failure modes, and how to mitigate them? (45)</div>	<div>Latent Sector Error - damaged group of sectors, detected by disk (eg head crash, cosmic rays) Fix - use redundancy to reconstruct disk</div><br><div>Block corruption - block becomes corrupt in a way not detectable by disk itself (eg buggy disk firmware) Fix - store a checksum per block to detect on read/write</div><br><div>Misdirected writes - data written to the wrong location. Fix - store a physical id alongside the checksum (another form of redundancy). Check the id value when reading to detect writing to the correct block.</div><br><div>Lost writes - When the write failed to execute. Fix - store the checksum in the inode as well as alongside the block (eg ZFS does this)</div><br><div>Occasional scrubbing can be performed as well, to check all of the checksums without slowing down each read write.</div><br>	OSTEP
<div>What is a dist shared memory (DSM) system? Why is leveraging OS abstractions in this way a poor choice for building dist systems? (48.4)</div>	<div>DSM enables processes on different machines to share a large, virtual address space. Turns a dist system into something that looks like a multi-threaded app - the only difference is that the threads run on different machines instead of different processes within the same machine.</div><br><div>They work through a virtual memory system of the OS. Eg: When a page is accessed on one machine, it’ll either (1) read from local machine’s cache or (2) page fault, then perform a remote call to fetch/install it into the page table of requesting process.</div><br><div>2 Downsides:&nbsp;</div><br><div>&nbsp;- How do we handle failure? If a machine fails, what happens to pages on that machine? Imagine if the next pointer in a LL points to a portion of addr space that is gone!</div><div>&nbsp;- How do we avoid perf hits? We assume memory access is cheap, but in this case, it becomes expensive.</div><div><br></div><div>Because of these downsides, DSM never made it very far and wasn't well adopted.</div><br>	OSTEP
<div>What is a Remote Procedure Call (RPC)? 4 issues it must deal with? (48.5)</div>	<div>On the client:</div><ol><li><div>Create a message buffer</div></li><li><div>Packe the info into the message buffer</div></li><li><div>Send the message to the destination RPC server (aka “marshalling of args” or “serialization of the message”)</div></li><li><div>Wait for reply</div></li><li><div>Unpack return code and other args (unmarshalling/deserialization)</div></li><li><div>Return to caller</div></li></ol><div>On server:</div><ol><li><div>Unpack the message (unmarshalling/deserialization)</div></li><li><div>Call into actual function</div></li><li><div>Package the results - return arg(s) are marshaled back into a single reply buffer</div></li><li><div>Send the reply</div></li></ol><br><div>4 Issues that RPC must deal with:</div><div>1. Which transport-level protocol to use</div><div>2. Handling large args (frag and reassembly)</div><div>3. Byte ordering</div><div>4. Synch vs async</div><br>	OSTEP
<div>What does it mean for an operation to be idempotent? How is this useful for distributed file systems like NFS? Examples? (49.7, summary)</div>	<div>Idempotence is the ability for a client to simply retry the request (regardless of what caused the failure). Thus, the effect of performing the operation multiple times is the same as the effect of performing the operation a single time.</div><br><div>This is important for NFS because it’s much easier to handle failure of an operation if you can just retry it.</div><br><div>Idempotent examples:</div><div>Storing a value in memory</div><div>Any operation that only reads data</div><br><div>Non-idempotent examples:</div><div>Incrementing a counter</div><br>	OSTEP
<div>What is Voltaire's law? How does it apply to distributed systems? (49.7)</div>	<div>Even when you design a beautiful system, sometimes all the corner cases don’t work out exactly as you might like. For example, you can redesign the mkdir NFS operation to be idempotent using different semantics (eg: handling timeouts in a unified way), but why bother? The NFS design philosophy covers most of the important cases, and overall makes the system design clean and simple wrt failure.</div><br><div>Thus, accepting that life isn’t perfect and still building the system is a sign of good engineering.</div><br><div>“The best is the enemy of the good”</div><br>	OSTEP
<div>What is the Cache Consistency Problem? 2 of its subproblems and their solutions? NFS vs AFS? (49.9, 50.5)</div>	<div><b>Update visibility problem</b> - when do updates from one client become visible at other clients?</div><div>This can be addressed with <b>flush-on-close (aka close-to-open)</b>&nbsp;consistency semantics, where the client flushes all updates (ie dirty pages in the cache) when a file is written to and subsequently closed.</div><br><div><b>Stale cache problem</b> - when a client has a stale version in its cache. Can be addressed by <b>invalidation</b>&nbsp;- having the client check to see whether a file has changed before using its cached contents. This is done in NFS via GETATTR request, where the client compares the timestamps of the file in cache vs server, and invalides the file if the server’s version is newer.</div><br><div>Note that the stale cache problem can also be addressed by registering <b>callbacks</b> to have the server notify the client when the subscribed file is no longer valid. This can avoid excessive GETATTR requests</div><br>	OSTEP
<div>What is a callback? How does AFS improve over NFS wrt callbacks? What other tradeoff is this analogous to? (50.4)</div>	A callback is a promise from the server to the client that the server will inform the client when a file that the client is caching has been modified. By adding state to the system, the client no longer needs to contact the server to find out if a cached file is still valid. Rather, it assumes that the file is valid until the server tells it otherwise (analogous to polling vs interrupts)<div><br></div><div>By minimizing server interactions (through whole-file caching and callbacks), the AFS protocol enables each server&nbsp; to support many clients compared with NFS. The consistency model provided by AFS is simple to understand and reason about, and does not lead to the occasional weird behavior as one sometimes observes in NFS.</div>	OSTEP
What is fractional binary notation? What does a w-length array of 1's equal in fractional binary? (2.4.1)<div><br></div>	<div>Given a bit vector of length i, the fractional binary value is:</div><div><br></div><div>[latex]\begin{displaymath}\sum_{i = -n}^{m}2^i * b_i \end{displaymath}[/latex]<br></div><div><br></div><div><div>Fractional binary notation represented as x * 2^y</div><div>Other values only approximated.</div><br><div>1/5 is 0.2 in dec, but approx</div><div>0.00110011 (51/256)</div><br><div>all 1s bit array = 1 - 2^(-w)</div></div>	chapter2 CSAPP
<div>How many bits are stored in the IEEE standard 754 for float and double? (2.4.2)</div>	<div>float: 1 sign, 8 exponent, and 23 precision bits</div><div>double: 1 sign, 11 exponent, and 52 precision bits<br> <br>Note that a 16-bit floating point is in the works!<br></div>	chapter2 CSAPP
<div>What are text files vs binary files? (1.1)</div>	<div>Text files consist exclusively of ASCII characters. All other files are binary files.</div>	chapter2 CSAPP
<div>4 phases of a compilation system? What are the input/output files in each phase? (1.3)</div>	<ol><li><div>Preprocessing phase: The preprocessor (cpp) modifies the original C program according to directives that begin with the ‘#’ character (eg: “#include &lt;stdio.h&gt;). The result is another C program, typically with the .i suffix.<br><br></div></li><li><div>Compilation phase: The compiler (cc1) translates the text file hello.i into the text file hello.s which contains an assembly-language program.<br><br></div></li><li>2. Assembly phase: Next, the assembler (as) translates the hello.s into machine-language instructions, packages them in a form known as a relocatable object program like hello.o. This is a binary file.<br><br></li><li><div>Linking phase: The linker (ld) handles merging with other precompiled object files (eg: printf.o for the printf function). The result is an executable object file (ie _executable) that is ready to be loaded into memory and executed by the system. (eg a .out file)</div></li></ol>	chapter2 CSAPP
<div>What is the kernel? (1.7.1)</div>	<div>The kernel is the portion of the os code that is always resident in memory. When an app program requires some action by the OS (eg: read or write a file), it executes a syscall instruction, transferring control to the kernel. The kernel then performs the requested operation and returns back to the app program.&nbsp;</div><br><div>Note that the kernel is not a separate process. Instead, it is a collection of code and data structures that the system uses to manage all the processes.</div>	chapter2 CSAPP
5 areas of a virtual address space? (1.7.3)<br>	<ol><li><div>Program code and data</div></li><li><div>Heap: expands and contracts dynamically as a result of calls to malloc and free</div></li><li><div>Shared libraries: eg: the C standard library and the math library</div></li><li><div>Stack: At the top of the user’s virtual address space, used by the compiler to implement function calls. Expands and contracts dynamically with functions calls.</div></li><li><div>Kernel virtual memory: Top region of the address space; not allowed access by app programs. The kernel must be invoked to read/write the contents of this area.</div></li></ol>	chapter2 CSAPP
<div>What is Multics, Unix, and Posix? (1.7.1)</div>	<div>Multics used a hierarchical file system and a shell as a user-level process, later borrowed by Unix but in a smaller, simpler package. Multics dragged on for years but never achieved wide-spread use due to complexity.</div><br><div>Unix, a Bell labs project, was announced in 1974 with a kernel re-written in C. The source code was made available to schools with generous terms, and developed a large following at universities.</div><br><div>In the 80’s Unix vendors tried to differentiate themselves by adding new and often incompatible features. To combat this trend, IEEE sponsored an effort to standardize Unix, later dubbed “Posix” by Stallman.&nbsp;</div><br><div>Posix standards cover issues as the C language interface for Unix syscalls, shell programs and utilities, threads, and network programming. Since then, differences between Unix versions have largely disappeared.</div>	chapter2 CSAPP
<div>What is Amdahl's law? (1.9.1)</div>	<div>When we speed up one part of a system, the effect on the overall system performance depends on both how significant this part was and how much it sped up.</div><br><div>(Told / T/new) = S = 1 / ((1 - a) + a/k)</div><br><div>where a is the proportion of the system, which can be sped up by a factor of k.</div>	chapter2 CSAPP
2 types of byte ordering? <br><br>What does it look like for a given value 0x01234567 stored at addr 0x10?<br><br>When is it important (3 cases)? (2.1.3)	<div>Oracle/IBM (big endian)</div><div>10 11 12 13</div><div>01 23 45 67</div><br><div>Intel (little endian)</div><div>10 11 12 13</div><div>67 45 23 01</div><br><div>Important to consider byte ordering hen:</div><div>1. sending binary data over a network</div><div>2. looking at byte sequences representing int data</div><div>3. writing programs that circumvent the normal type system</div>	chapter2 CSAPP
"<div>How is the ASCII string ""12345"" represented in memory? How does byte ordering and word size affect the representation? (2.1.4)</div>"	<div>1 &nbsp; 2 &nbsp; 3 &nbsp; 4 &nbsp; 5 &nbsp; NULL</div><div>31 32 33 34 35 00</div><div>(note that the ASCII code for decimal digit x happens to be 0x3x, and that the terminating byte has hex representation 0x00)</div><br><div>The representation is unaffected by byte ordering and word size conventions. As a consequence, text data are more platform independent than binary data.</div>	chapter2 CSAPP
"<div>What bool expression is the XOR bool operation equivalent to? IOW how can we rewrite ""x ^ y""? (2.1.7)</div>"	<div>x ^ y = (x &amp; ~y) | (~x &amp; y)</div><br><div>1st group retains only 1s where x is 1 and y is 0.</div><div>2nd group retains only 1s where y is 1 and x is 0.</div><div>“Or” them together to get the xor.</div>	chapter2 CSAPP
<div>What is a bijection? How does it apply to representing information in binary? (2.2.2)</div>	<div>Refers to a function f that goes two ways: it maps a value x to a value y where y=f(x), but it can also operate in reverse, since for every y, there is a unique value x such that f(x) = y. This is given by an inverse function, f_2, where x=f_2(y) for our example.</div><br><div>Converting binary numbers to unsigned and two’s complement are examples:</div><br><div>B2U with inverse U2B</div><div>B2T with inverse T2B</div><div>U2T with inverse T2U</div>	chapter2 CSAPP
<div>What is the principle for converting from 2's complement to unsigned? (2.2.4)</div>	"<div>Where w is the bit length of the word:</div><br><div>T2Uw(x) =&nbsp;</div><div>x + 2^w&nbsp; (when x &lt; 0)</div><div>x&nbsp; (when x &gt;=0)</div><br><div>Some useful conversions:</div><br><div>U2T(Umax) = -1</div><div>U2B(Umax) = OxFF...</div><div>B2T(0xFF..) = Umax</div><div>T2Uw(Tminw) = -2^(w-1) + 2^w = 2^(w-1)=Tmaxw + 1</div><br><div>When casting between unsigned and signed:</div><br><div>int x = -1;</div><div>unsigned u = 2147483648; /* 2 to the 31st */</div><br><div>printf(""x = %u = %d\n"", x, x);</div><div>printf(""u = %u = %d\n"", u, u);</div><br><div>&gt; x = 4294967295 = -1</div><div>&gt; u = 2147483648 = -2147483648</div><br>"	chapter2 CSAPP
<div>What is zero extension and sign extension? How does Java handle sign extensions differently? When are unsigned words useful? (2.2.6, 2.2.8)</div>	<div>To convert an unsigned number to a larger data type, we can simply add leading zeros to the representation (aka “logical shift”)</div><br><div>For converting a two's-complement number to a larger data type, the rule is to perform a _sign extension_, adding copies of the most significant bit to the representation. Note that sign extension preserves the value of a two's-complement number (aka “arithmetic shift”)</div><br><div>Note that Java doesn't have unsigned numbers - everything is signed. Eg x - y &lt; 0 will always be safe.</div><br><div>&gt;&gt; is always an arithmetic shift, and &gt;&gt;&gt; is a logical shift.</div><br><div>It simplifies things, but unsigned values are useful when thinking of words as collections of bits w no numeric interpretation.</div><br><div>Sign extension examples:</div><br><div>for w = 3</div><div>[101] = -4 + 1 = -3</div><div>sign extended to w=4:</div><div>[1101] = -8 + 7 + 1 = -3</div><div>[11101] = -16 + 8 + 7 + 1 = -3</div><div>Still equals -3!</div><br><div>2^w - 2^(w-1) = 2^(w-1)</div><div>Thus the combined effect of adding a bit of weight -2^w and of converting the bit having weight -2^(w-1) to be one with weight 2^(w-1) is to preserve the original numeric value.</div>	chapter2 CSAPP
What is an additive inverse (aka negation), and how do we calculate it for unsigned and two's complement numbers? (probs 2.33, 2.29)	<div>For a number x, the additive inverse, y, is such that:</div><br><div>x + y = 0</div><br><div>For unsigned numbers, it is simply:</div><div>y = UMax - x</div><br><div>For two’s complement, it is:</div><div>TMin when x == TMin</div><div>Or else</div><div>-x</div><br><div>Some examples:</div><div>dec hex dec hex</div><div>-8 8 -8 8 (two’s complement)</div><div>8 8 8 8 (unsigned)</div><br><div>5 5 -5 B (t)</div><div>5 5 11 B (u)</div><br><div>-3 D 3 3 (t)</div><div>13 D 3 3 (u)</div><br><div>-1 F 1 1 (t)</div><div>15 F 1 1 (u)</div><br><div>Note that the bit patterns (hex values) are the same after negating the same value for either unsigned or two’s complement.</div>	chapter2 CSAPP
How does a compiler calculate x * K? (assume x is a bit vector, and K is a constant) (2.3.6)	<div>The compiler can represent K as an alternating sequence of 1’s and 0’s:</div><div>[(0..0)(1..1)(0..0)...(1..1)]</div><br><div>Consider a run of ones from bit position n down to bit position m (n&gt;=m). The effect of these bits on the product can be in 2 form:</div><div>Form A: (x&lt;&lt;n ) + (x&lt;&lt;(n-1)) + .. + (x&lt;&lt;m)</div><div>Form B: (x&lt;&lt;(n+1)) - (x&lt;&lt;m)</div><br><div>Eg: a compiler can optimize:</div><div>7*x to be:</div><div>(x&lt;&lt;2) + (x&lt;&lt;1) + x</div><div>or:</div><div>&nbsp;(x&lt;&lt;3) - x</div><br><div>By adding together the results of each run, we can compute x * K without any multiplications. Of course, the tradeoff between using combinations of shifting, adding, and subtracting vs a single multiplication instruction depends on the relative speed of the instructions, and can be machine-dependent. Most compilers only perform this optimizations when a small number of shifts, adds, and subtractions suffice.</div>	chapter2 CSAPP
<div>Definition of int division? How do we achieve it in both signed and unsigned? What is one wrinkle solved by bias? (2.3.7)</div><br>	<div>Integer division behaves like regular division but always rounds towards 0. That is, it should round down a positive result but round up a negative one.</div><div>This extends to using logical shifts for unsigned numbers and arithmetic shifts for signed.</div><br><div>Note how unsigned and two's complement have very similar bit-level behaviors for mult, add, sub, and div</div><br><div>Unfortunately right shifting a negative two’s complement number will cause it to improperly round down. This is solved by “biasing” the value before shifting by adding (2^k) - 1 to the number before shifting, where k is the number of bits to shift.&nbsp;</div><br><div>For example, in C, when dividing an i32 by 16 (k = 4), we can conditionally assign the bias via:</div><br><div>// bias is 15, but is 0 when x is non-negative</div><div>int bias = (x&gt;&gt;31) &amp;&amp; 0xF;</div><div>return (x + bias) &lt;&lt; 4;</div>	chapter2 CSAPP
How are IEEE standard 754 float point numbers represented, and their 3 categories? Where is a bias value used? (2.4.2)	"<div>Value V = (-1)^s * M * 2^E</div><br><div>s is the ""sign"" (1 bit)</div><div>M is the ""significand"", a fractional binary (23 bits single, 52 bits double)</div><div>E is the ""exponent"" (8 bits single, 11 bits double)</div><br><div>The bit representation is divided into 3 fields to encode the values:</div><div>1. sign bit s</div><div>2. k-bit exponent field, e, encodes exponent E</div><div>3. n-bit fraction field, f, encodes the significand M (but depends on whether the exponent field is 0)</div><br><div>Note that we use a bias to translate the range:</div><div>Bias = 2^(k-1) - 1 (-126 to +127 for single precision, and -1022 to + 1023 for double precision)</div><br><div>There are 3 ways an IEEE bits are interpreted:</div><div>*Normalized*</div><div>0 &lt;= f &lt; 1</div><div>M = 1+f (an ""implied leading 1"" representation, since we always adjust E to make 1 &lt;= M &lt; 2, saving an extra bit of precision)</div><div>E = (unsigned exponent field) - bias</div><br><div>*Denormalized*</div><div>M = f</div><div>E = 1 - bias</div><br><div>2 purposes of the denormalized type:</div><div>1. repr 0 (as +0.0 and -0.0)</div><div>2. Repr numbers very close to 0 via gradual underflow</div><br><div>*Special*</div><div>+Inf: All 1s except sign bit</div><div>-Inf: All 1s</div><div>NaN: All 1s in exponent, something other than 0s in f</div><br><div>Some notable values:</div><div>s e&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;f</div><div>0 0.....0 0..01 = smallest denormalized (E=-2^(k-1) + 2, M=f=2^(-n))</div><div>0 0.....0 1...1 = largest denormalized (E=-2^(k-1) + 2, M=f=1-2^(-n))</div><div>0 0...01 0...0 = smallest normalized (E = -2^(k-1) + 2, M=1+f=1)</div><div>0 01...1 0...0 = 1 (E = 0, M=0)</div><div>0 1.....1 1...1 = largest normalized (E=2^(k-1) - 1, M=2-2^(-n))</div><div>1 1.....1 0...0 = +inf</div><div>0 1.....1 0...0 = -inf</div><div>1 0.....0 0...0 = - zero</div><div>0 0.....0 0...0 = +zero</div><br><div>""underflow"" is when the float is so close to 0 that they are changed to zero</div><br>"	chapter2 CSAPP
<div>What is round-to-even (aka round-to-nearest) and how is it used by IEEE standard 754 floating point standard? (2.4.4)</div>	<div>Round-to-even is the default mode for floating point, and attempts to find the closest match. The only design decision is to determine the effect of rounding values that are halfway between two possible results. Round-to-even mode adopts the convention that it rounds the number either upward or downward such that the least significant digit of the result is even. (eg: it rounds both 1.5 and 2.5 to 2)</div><br><div>The goal of round-to-even is to avoid introducing statistical bias into the computation of the average values. Eg: if we always rounded halfway values upward, the average would be slightly higher than the average of the numbers themselves.</div><br><div>More examples, when rounding to the nearest half (base 2):</div><div>10.010 (2 1/4) =&gt; 10.0 (rounded down)</div><div>10.011 (2 3/8) =&gt; 10.1 (nearest)</div><div>10.110 (2 3/4) =&gt; 11.0 (rounded up)</div><div>11.001 (3 1/8) =&gt; 11.0 (rounded down)</div><br>	chapter2 CSAPP
<div>How do floating point operations compare to their unsigned/two's complement equivalents wrt associative, commutative, and monotonic properties? Consider the addition and multiplication operations.  (2.4.5)</div>	<div>Addition is commutative and associative for unsigned/two's complement, because they form abelian groups. Not monotonic.</div><br><div>In fp, addition is only commutative, and forms an abelian group. But it is not associative:</div><div>(3.14+1e10) - 1e10 = 0.0 because the 3.14 is lost due to rounding; while</div><div>3.14 + (1e10 - 1e10) = 3.14</div><br><div>This lack of associativity is the most important property that it's lacking.and causes trouble for compiler writers.</div><br><div>fp addition satisfies monotonicity: if a &gt;= b, then x + a &gt;= x + b</div><div>This is not true of unsigned or two's complement addition due to overflow.</div><br><div>Multiplicity is also commutative and not associative for fp, due to the possibility of overflow or the loss of precision due to rounding.</div><div>Eg on single precision fp:</div><div>(1e2*1e20)*1e-20 = +inf while</div><div>1e20*(1e20*1e-20) = 1e20</div><br><div>Fp multiplication also does not distribute over addition:</div><div>1e20*(1e20-1e20) = 0.0 while</div><div>1e20*1e20-1e20*1e20 = NaN</div><br><div>fp multiplication does satisfy monotonicity properties other than NaN:</div><div>a &gt;= b and c &gt;= 0 then a * c &gt;= b*c</div><div>a &gt;= b and c &lt;= 0 then a*c &lt;= b*c</div><div>a*a &gt;= 0 as long as a != NaN</div>	chapter2 CSAPP
<div>What are the concerns when casting: int to a float? Double to a float? Int or float to double? Float or double to int? (assume 32-bit int) (2.4.6)</div>	<div><b>int to float</b>: the number cannot overflow, but can be rounded</div><br><div><b>int or float to double</b>: the exact numeric value can be preserved because double has both greater range and greater precision (ie: thue number of significant bits)</div><br><div><b>double to float</b>: the value can overflow to +/- INFINITY, or may be rounded, because both precision and range is smaller</div><br><div><b>float or double to int</b>: The value will be rounded toward 0. It may also overflow, although the C standards do not specify a fixed result for this case. Intel designates the bit pattern [10..0] (TMin) as an _integer indefinite_ value. Eg: (int) +1e10 yields -21483648, generating a negative value from a positive one.</div><br><div>Eg, assuming x, f, and d are int, float, and double respectively. f and d can't be +/- INFINITY or NaN.</div><br><div>x == (int)(double) x</div><div>f == (float) (double) f</div><div>f == -(-f)</div><div>1.0/2 == 1/2.0</div><div>d*d &gt;= 0.0</div><br><div>Note that C implementations follow the convention that the underlying bit patterns are unchanged when converting between signed and unsigned.</div><br><div>Eg assuming x and y are i32:</div><div>x+y == (unsigned x) + (unsigned y)</div>	chapter2 CSAPP
<div>What are 4 parts of the processor state normally hidden from a C programmer? (3.2.1)</div>	<ol><li><div>The program counter (aka the PC, and called %rip in x86-64) indicates the address in memory of the next instruction to be executed.<br><br></div></li><li><div>The integer _register file_ contains 16 named locations storing 64-bit values. These registers can hold addresses (corresponding to C pointers) or integer data. Some registers are used to keep track of critical parts of the program state, while others are used to hold temporary data, such as the arguments and local variables of a procedure, as well as the value to be returned by a function.<br><br></div></li><li><div>The condition code registers hold status info about the most recently executed arithmetic or logical instruction. These are used to implement conditional changes in the control or data flow, such as is required to implement if and while statements.<br><br></div></li><li><div>A set of vector registers can each hold one or more integer or floating-point values.</div></li></ol><br><div>Whereas C provides a model in which objects of different data types can be declared and allocated, machine code views the memory as simply a large byte-addressable array. Eg: arrays and structures are represented in machine code as contiguous collections of bytes. Even for scalar data types, asm makes no distinctions between signed or unsigned integers, between different types of pointers, or even between pointers and integers.</div>	chapter3 CSAPP
<div>What are some differences between Intel and ATT asm formats (4 differences)? (3.2.3)</div>	<ol><li><div>The Intel code omits the size designation suffixes. Eg: We see instruction push and mov instead of pushq and movq<br><br></div></li><li><div>The Intel code omits the % character in front of register names, using rbx instead of %rbx<br><br></div></li><li><div>The Intel code has a different way of describing location in memory - for example, QWORD PTR[rbx] rather than (%rbx)<br><br></div></li><li><div>Instructions with multiple operands list them in reverse order. This can be very confusing when switching between the two formats.</div></li></ol>	chapter3 CSAPP
<div>What are the 2 ways to add asm into C code? (3.3)</div>	<ol><li><div>We can write an entire function as a separate assembly-code file and let the assembler and linker combine this with code we have written in C.<br><br></div></li><li><div>We can use the _inline assembly_ feature of GCC, where brief sections of assembly code can be incorporated into a C program using the “asm” directive. This approach has the advantage that it minimizes the amount of machine-specific code. (of course, this makes the code specific to a particular class of machines)</div></li></ol><div>Note that asm does not keep track of the type of a program value. Instead, different instructions determine the operand sizes and whether they are signed/unsigned.</div>	chapter3 CSAPP
<div>What are the 7 C data type sizes in x86-64? Include the C declarations, Intel data type, and asm code prefix for each (3.3)</div>	<pre>| C declaration | Intel data type  | Assembly Code Suffix | Size (bytes) | |---------------+------------------+----------------------+--------------| | char          | Byte             | b                    |            1 | | short         | Word             | w                    |            2 | | int           | Double word      | l                    |            4 | | long          | Quad word        | q                    |            8 | | char *        | Quad word        | q                    |            8 | | float         | Single precision | s                    |            4 | | double        | Double precision | l                    |            8 |  </pre> 	chapter3 CSAPP
<div>Name all of the x86-64 registers, along with their lower-order portions (figure 3.2)<br>Don't forget about the SIMD registers too.</div>	<pre>| q   | l    | w    | b    | use           |<br>|-----+------+------+------+---------------|<br>| rax | eax  | ax   | a1   | return value  |<br>| rbx | ebx  | bx   | b1   | callee saved  |<br>| rcx | ecx  | cx   | c1   | 4th arg       |<br>| rdx | edx  | dx   | d1   | 3rd arg       |<br>| rsi | esi  | si   | si1  | 2nd arg       |<br>| rdi | edi  | di   | di1  | 1st arg       |<br>| rbp | ebp  | bp   | bp1  | callee saved  |<br>| rsp | esp  | sp   | sp1  | stack pointer |<br>| r8  | r8d  | r8w  | r8b  | callee saved  |<br>| r9  | r9d  | r9w  | r9b  | callee saved  |<br>| r10 | r10d |      |      |    5th arg    |<br>| r11 | r10d |      |      |    6th arg    |<br>| …   |      |      |      | callee saved  |<br>| r15 | r15d | r15w | r15b | callee saved  |<br></pre><br><p><br>There are also 16 AVX registers, %ymm0-%ymm15. We typically ony use their lower-order portions, %xmm0-%xmm15, for storing floats and doubles. These are all caller-saved registers (aka the callee may overwrite any of these registers without first saving it)</p>	chapter3 CSAPP
<div>What are 4 kinds of mov asm instructions? When do we use each? (3.4.2) Why don't we have a movzlq instruction?</div>	"<div>mov - copies data from source to destination w/o transformation. 4 sizes: movb, movw, movl, movq.</div><br><div>Use these when copying a smaller source value to a larger destination, and fill in the leading bits:</div><div>movz - zero-extending data movement. Has 5 types:</div><div>movzbw</div><div>movzbl</div><div>movzwl</div><div>movzbq</div><div>movzwq</div><br><div>movs - sign-extending data movement. Has 6 types:</div><div>movsbw</div><div>movsbl</div><div>movswl</div><div>movsbq</div><div>movswq</div><div>movslq (note this doesn't exist on movz, becuase movl auto-clear the leading bytes by convention)</div><div>cltq (same as ""movslq %eax %rax"", but has more compact encoding)</div><br><div>cmov - copies the source value to its destination when the condition holds. Is one of 3 ways that condition codes are read (the other two are the “set” and “jump” instructions)</div><br><br><div>Note that conditional data transfers (cmov) often outperform conditional control transfers (via jmp), due to pipelining overlapping the steps of successive instructions.</div><ul><li><div>Usually only used when the two expressions in the ternary operator can be computed easily.</div></li></ul><br><br><div>Note that these mov instructions cannot have both operands be memory locations, as it requires an extra register to store the intermediate value.</div><br><div>Note that references to memory locations are always given w quad word registers, like %rax, even if the operand is b, w or l.</div>"	chapter3 CSAPP
<div>What is processor pipelining, and how does it help conditional data transfers to outperform conditional control transfers?<br><br>What are 2 limitations of conditional data transfer optimization?<br><br>What's the name of the sophisticated technique used to mitigate cost of conditional control transfers? (3.6.6)</div>	<div>Processors achieve high performance through pipelining - where an instruction is processed via a sequence of stages, each performing one small portion of the required operations (eg: fetching the instruction from memory, determining the instruction type, reading from memory, performing an arithmetic operation, writing to memory, and updating the program counter).</div><br><div>This approach achieves high performance by overlapping the stages of the successive instructions, such as fetching one instruction while performing the arithmetic operation for a previous instruction.</div><br><div>For a conditional jump, processors employ sophisticated _branch prediction logic_ to guess whether or not each jump instruction will be followed. As long as it can guess reliably, the pipeline will be kept full of instructions. Mispredicting a jump, otoh, requires the processor to discard that work already done on future instructions and then begin filling the pipeline with instructions starting at the correct location. This can be a serious penalty (eg: 15-30 clock cycles)</div><br><div>Conditional moves only require ~8 clock cycles, regardless of the data being tested. Flow of control does not depend on the data, and this makes it easier for the processor to keep the pipeline full.</div><br><div>Eg:</div><br><div>given:</div><div>v = test-expr ? then-expr : else-expr;</div><br><div>The standard way to compile this is by using conditional control transfer (ie: if/else expression), ensuring that just one of the sequences is evaluated. But if we guess wrong, we’ll lose all that work!</div><div>For the conditional move, both the then-expr and the else-expr are evaluated, with the final value chosen based on the evaluation test-expr. Eg:</div><br><div>V = then-expr;</div><div>Ve = else-expr;</div><div>T = test-expr;</div><div>If (!t) v = ve;</div><br><div>The final statement is done with a conditional move - copying ve into v iff test condition t does not hold. This ensures that we pay a small fixed cost, and will have both branches evaluated!</div><br><div>2 limitations:</div><div>This approach only works when neither branch has side-effects.</div><div>It only is used when neither branch is expensive to calculate. (experiments with gcc indicate that it only uses conditional moves when the two expressions can be computed very easily, like single add instructions)</div><br><div>Although it can only be used in restricted cases, these cases are fairly common and provide a much better match to the operation of modern processors.</div>	chapter3 CSAPP
"<div>If ""long *xp"" is in %rdi, and ""long y"" is in %rsi, what instructions are equivalent to ""long x = *xp; *xp = y; return x;""? How is pointer “*xp”, and local variable ""x"", stored? (3.4.3)</div>"	"<div>movq (%rdi), %rax</div><div>movq %rsi, (%rdi)</div><div>ret</div><br><div>2 features worth noting:</div><div>1. ""pointers"" in C are simply addresses. Dereferencing a pointer involves copying that pointer into a register, and then using this register as a memory reference (in this case %rdi)</div><div>2. Local variables such as x are often kept in registers rather than stored in memory locations, because register access is much faster than memory access (in this case %rax)</div>"	chapter3 CSAPP
What is the lea instruction, and how is it different than mov? (3.5.1)	"The _load effective address_ instruction leaq is actually a variant of the movq instruction. It reads from memory to a register, but does not reference memory at all. Its first operand appears to be a memory reference, but instead of reading from the designated location, the instruction copies the effective address to the destination. This instruction can be used to generate pointers for later memory references.  Additionally, the ability of leaq inst to perform addition and subtraction and limited forms of multiplication proves useful when compiling simple arith expressions. Eg: if %rdx contains value x, then “leaq 7(%rdx,%rdx,4), %rax” will set %rax to 5x + 7.  Modern compilers think of it as just another option for doing arithmetic on pointers / integers, and that's how you should think of it, too.  For example, the following expressions have the same result:  <pre>mov ax,[bx+si+5] lea ax,[bx+si+5] </pre>  In short, LEA loads a pointer to the item you're addressing whereas MOV loads the actual value at that address. The purpose of LEA is to allow one to perform a non-trivial address calculation and store the result [for later usage], Eg: <pre>leaq 5(%rbp, rsi), %rax  // computes address of value movq 5(%rbp, rsi), %rax  // loads the value at that address </pre>  The leaq instruction above would have to be written in multiple instructions, like “add” or “mul” to avoid loading the actual value. Lea also doesn’t alter the ZF and CF condition codes while computing the address by arithmetic instructions, like “add” or “mul” does. This feature decreases the level of dependency among instructions and thus makes room for further optimization by the compiler or hardware scheduler.  When compiled, arithmetic operations are implemented by a sequence of leaq operations.  Leaq is also used for 8-byte operations dealing with pointers (3.8.2): Eg when storing references into other registers:  <pre>subq $16 %rsp // allocate an 16 byte stack frame movb $4, 15(%rsp) // store 4 in a reference (eg: &amp;x) leaq 15(%rsp), %rax // creates &amp;x </pre> Note that we still have 15 free bytes left of the stack frame, and instead of storing the value 4 (aka ""x"") into its own register, we save it on the stack. "	chapter3 CSAPP
What are the 4 types of integer and logic asm instructions? (3.5)	<ol><li><div>Load effective address - leaq</div></li><li><div>Unary - inc / dec / neg / not</div></li><li><div>Binary - add / sub / imul / xort / or / and</div></li><li><div>Shifts - sal / shl / sar / shr</div></li></ol>	chapter3 CSAPP
<div>How does asm support multiplying two 64-bit signed or unsigned integers to yield a product that requires 128 bits to represent? Or dividing two 128-bit integers? (3.5.5)</div>	x86-64 provides limited support for operations involving 128-bit (16-byte) numbers. Intel refers to a 16-byte quantity as an oct word. The pair of registers $rdx and $rax are viewed as forming a single 128-bit oct word.  <pre>| instr | effect                                 | description            | |-------+----------------------------------------+------------------------| | imulq | R[$rdx]:R[$rax] &lt;- S x R[%rax]         | signed full multiply   | | mulq  | (same as above)                        | unsigned full multiply | | cqto  | $[%rdx]:R[%rax] &lt;- SignExtend(R[%rax]) | Convert to oct word    | | idivq | R[%rdx] &lt;- R[%rdx]:R[%rax] mod S       | Signed divide          | |       | R[%rax] &lt;- R[%rdx:R[%rax] / S          |                        | | divq  | (same as above)                        | Unsigned divide        | </pre>  Notice: 1. for division we are storing the remainder in %rdx and the quotient in %rax. 2. the unsigned and signed operations have the same bitwise effect  Eg for representing unsigned 128-bit multiplication as asm: <pre>void store_uprod(uint128_t *dest, uint64_t x, uint64_t y) {   *dest = x * (uint128_t) y; }  // in asm: // (dest in %rdi, x in %rsi, y in %rdx) store_uprod:   movq %rsi, %rax    ; copy x to multiplicand   mulq %rdx          ; multiply by y   movq %rax, (%rdi)  ; store lower 8 bytes at dest   movq %rdx, 8(%rdi) ; store upper 8 bytes at dest+8  </pre>	chapter3 CSAPP
<div>What are 4 condition codes in asm? And 3 common ways for condition codes to get set? 3 common ways for them to be read? (3.6.1)</div>	"<div>CF: Carry Flag - The most recent operation generated a carry out of the most sig bit. Used to detect overflow for unsigned operations.</div><div>ZF: Zero Flag - The most recent operation yielded zero.</div><div>SF: SIgn flag: the most recent operation yielded a neg value.</div><div>OF: Overflow flag: the most recent operation caused a two's-complement overflow - either neg or positive.</div><br><div>For example, given an add instruction that does “t = a + b”, we’ll have:</div><div>CF: (unsigned) t &lt; (unsigned) a</div><div>ZF: (t == 0)</div><div>SF: (t &lt; 0)</div><div>OF: (a &lt; 0 == b &lt; 0) &amp;&amp; (t &lt; 0 != a &lt; 0)</div><br><div>3 instruction classes that set condition codes without setting other registers:</div><div>- cmp S1 S2 is based on S2 - S1</div><div>- test S1 S2 is based on S1 &amp; S2</div><div>&nbsp;- 3 of the 4 categories in our “integer and arithmetic instructions” (all of them but leaq: unary, binary, and shift operators)</div><br><div>There are 3 common ways of using the condition codes:</div><ol><li><div>We can set a single byte to 0 or 1 depending on some combination of the cond codes (the “set” instructions)</div></li><li><div>We can conditionally jump to some other part of the program (the “jump” instructions)</div></li><li><div>We can conditionally transfer data (the “cmov” instructions)</div></li></ol><br><div>Additional details on how condition codes are evaluated for various comparisons:</div><br><div>g - ""greater than, aka nle (signed &gt;)”: ~(SF ^ OF) &amp; ~ZF</div><div>ge - “greater or equal, aka nl (signed &gt;=)”: ~(SF ^ OF)</div><div>l - “less, aka nge (signed &lt;)”: SF ^ OF</div><div>le - “less or equal, aka ng (signed &lt;=)”: (SF ^ OF) | ZF</div><div>&nbsp;</div><div>a - ""above, aka nbe (unsigned &gt;)"": ~CF &amp; ~ZF</div><div>ae - “above or equal, aka nb (unsigned &gt;=)”: ~CF</div><div>b - “Below,&nbsp; aka nae (unsigned &lt;)”: CF</div><div>be - “below or equal, aka na (unsigned &lt;=)”: CF | ZF</div><br>"	chapter3 CSAPP
How do the “set” instructions use condition codes? (3.6.2)	They set a single byte to 1 or 0 dep on some combo of the condition codes.<br><br>Eg for a comparator function that returns 1 if a is greater than b:<br><pre>int comp(data_t a, data_t b) {...}<br><br>(a in %rdi, b in %rsi)<br>comp:<br>  cmpq %rsi, %rdi   ; compare a:b<br>  setl %al          ; set low order byte of %eax to 0 or 1<br>  movzbl %al, %eax  ; clear rest of %eax (and rest of %rax)<br>  ret<br></pre><br>	chapter3 CSAPP
<div>How can we calculate the base-10 and base-2 values of the two’s complement representation 0xff73?</div>	<pre><br># base 10:<br><br>(−16×16)+(7×16)+3<br>=-141<br><br># base 2:<br><br>1111 1111 0111 0011<br></pre>	chapter2 CSAPP
<div>What are 2 methods for encoding the target address of a jump instr? (3.6.4)</div>	<div>PC relative: Encodes the difference between the address of the target instruction and the address of the instruction immediately following the jump. Then can use 1, 2, or 4 bytes, which allows the object code to be shifted to different positions in memory without alteration.</div><br><div>Absolute: Uses 4 bytes to directly specify the target. This is a simpler approach, but less flexible and can cost more memory.</div><br><div>Note that the assembler and linker select the appropriate encodings of the jump destinations.</div>	chapter3 CSAPP
<div>What is the basic mechanism when translating loops into machine code? What are 2 ways to translate a while loop into machine code, and when should we use which? How do we translate for-loops? (3.6.7)</div>	Translating loops follows a simple strategy of generating code that contains one or more conditional branches. Conditional transform of control is the basic mechanism for translating loops into machine code.&nbsp;<div><br></div><div>&nbsp;*jump to middle:* performs the initial test by performing an unconditional jump to the test at the end of the loop.   jump-to-middle template on a while loop:  <pre>goto test; loop:   body-statement test:   t = test-expr;   if (t)     goto loop; </pre>  *guarded do:* This translation method first transforms the code into a do-while loop by using a conditional branch to skip over the loop if the initial test fails. Using this implementation strategy, the compiler can often optimize the initial test, for example, determining that the test condition will always hold.&nbsp;</div><div><br></div><div>Eg: guarded-do template on a while loop:  <pre>t = test-expr; if (!t)   goto done; loop:   body-statement   if (t)     goto loop done: </pre>  Examples with factorials: Jump to middle looks like the following goto code for calculating a factorial:  <pre>long fact_while_jm_goto(long n) {   long result = 1;   goto test; loop:   result *= n;   n = n - 1; test:   if (n &gt; 1)     goto loop;   return result; </pre>  Guarded-do goto code on a factorial:  <pre>long fact_while_gd_goto(long(n) {   long result = 1;   if (n &lt;= 1)     goto done; loop:   result *= n;   if (n != 1)     goto loop; done:   return result; } </pre>  *Translating for-loops* can be handled in a similar manner, since for-loops can be re-written as while loops.&nbsp;</div><div><br></div><div>Eg:  <pre>for (init-expr; test-expr; update-expr)   body-statement </pre>  is equivalent to this while loop:  <pre>init-expr; while (test-expr) {   body-statement   update-expr } </pre>  Thus has this jump-to-middle template:  <pre>  init-expr;   goto test; loop:   body-statement   update-expr; test:   t = test-expr;   if (t)     goto loop; </pre>  and this guarded do:  <pre>  init-expr;   t = test-expr;   if (!t)     goto done; loop:   body-statement   update-expr;   t = test-expr;   if (t)     goto loop; done: </pre> </div>	chapter3 CSAPP
How is a switch statement implemented in asm (3 things)? Why is it used? (3.6.8)	A switch statement provides a multiway branching capability based on the value of an integer index; useful when dealing with tests where there can be a large number of possible outcomes. They make C code more readable, and are more efficient than using a long sequence of if-else statements.<br><br><br>The key step in executing a switch statement is:<br><br>1. To access a code location through the _jump table_.<br><br>2. Which is a type of computed goto is supported by GCC as an extension to the C language.<br><br>3. In asm, this leverages the * operand to indicate an indirect jump.<br><br><br>Use of a jump table allows for a very efficient way to implement a multiway branch. A single jump table can handle access for even hundreds of cases.<br><br>Examples of C, goto, and asm for a switch/case compiled into a computed goto/jump table:<br><br><pre>void switch_eg(long x, long n, long *dest) {<br>  long val = x;<br>  switch (n) {<br>  case 100:<br>    val *= 13;<br>    break;<br>  case 102:<br>    val += 10;<br>    /* fall through */<br>  case 103:<br>    val += 11;<br>    break;<br>  case 104:<br>  case 106:<br>    val *= val;<br>    break;<br>  default:<br>    val = 0;<br>  }<br>  *dest = val;<br>}<br></pre><br><pre>void switch_eg_impl(long x, long n, long *dest) {<br>  /* table of code pointers */<br>  static void *jt[7] = {<br>    &amp;&amp;loc_A, &amp;&amp;loc_def, &amp;&amp;loc_B,<br>    &amp;&amp;loc_C, &amp;&amp;loc_D, &amp;&amp;loc_def,<br>    &amp;&amp;loc_D<br>  };<br>  unsigned long index = n - 100;<br>  long val;<br><br>  if (index &gt; 6)<br>    goto loc_def;<br>  /* multiway branch */<br>  goto *jt[index];<br><br>  loc_A: /*case 100 */<br>    val = x * 13;<br>    goto done;<br>  loc_B: /* case 102 */<br>    x = x _ 10;<br>    /* fall through */<br>  loc_C: /* case 103 */<br>    val = x _ 11;<br>    goto done;<br>  loc_D: /* cases 104, 106 */<br>    val = x*x;<br>    goto done;<br>  loc_def: /* default case */<br>    val = 0;<br>  done:<br>    *dest = val;<br>}<br></pre><br><pre>void switch_eg(long x, long n, long *dest)<br>x in %rdi, n in %rsi, dest in %rdx<br><br>switch_eg:<br>  subq $100, %rsi    ; compute index = n-100<br>  cmpq $6, %rsi      ; compare index:6<br>  ja .L8             ; if &gt;, goto loc_def<br>  jmp *.L4(,%rsi, 8) ; goto *jg[index]<br>.L3                  ; loc_A:<br>  leaq (%rdi, %rdi,2), %rax  ; 3*x<br>  leaq (%rdi,%rax,4), %rdi   ; val = 13*x<br>  jmp .l2                    ; goto done<br>.L5                         ; loc_B:<br>  addq $10, %rdi            ; x = x _ 10<br>.L6                         ; loc_C:<br>  addq $11, %rdi            ; val = x _ 11<br>  jmp .L2                   ; Goto done<br>.L7                         ; loc_D<br>  imulq %rdi, %rdi          ; val = x*x<br>  jmp .L2                   ; goto done<br>.L8                         ; loc_def<br>  movl $0, %edi             ; val = 0<br>.L2                         ; done:<br>  movq %rdi, (%rdx)         ; *dex = val<br>  ret                       ; return<br><br><br>.section .rodata   ; read-only data<br>.align 8      ; align address to multiple of 8<br>.L4:<br>  .quad .L3  ; Case 100: loc_A<br>  .quad .L8  ; Case 100: loc_def<br>  .quad .L5  ; Case 100: loc_B<br>  .quad .L6  ; Case 100: loc_C<br>  .quad .L7  ; Case 100: loc_D<br>  .quad .L8  ; Case 100: loc_def<br>  .quad .L7  ; Case 100: loc_D<br></pre><br>	chapter3 CSAPP
<div>What are 3 mechanisms we rely on for implementing procedures in machine code? 3 use cases for storing data on the stack? What are the 3 parts of the stack frame that these mechanisms use? When does a procedure not require a stack frame? (3.7.0)</div>	<div>Supposing procedure P call procedure Q, and Q then executes and returns back to P, we rely on one or more of the following 3 mechanisms:</div><br><div><b>passing control:</b> The program counter must be set to the starting address of the code for Q upon entry, and then set to the instruction P following the call to Q upon return.</div><br><div>When P calls Q, it will push the return address onto its stack frame, indicating where within P the program should resume execution once Q returns.</div><br><div><b>passing data:</b>&nbsp;P must be able to provide one or more parameters to Q, and Q must be able to return a value back to P. To do this, x86-64 allows integral (ie integer and pointer) args to be passed via registers rdi, rsi, rdx, rcx, r8, and r9 (and their lower-order portions), and the AVX registers %xmm0-%xmm7 for floating point.</div><br><div>When a function has more than six integral arguments, the other ones are passed on the stack. Thus, the code for P must allocate a stack frame with enough storage for arguments 7 through n (with arg 7 at the top of the stack). And if Q calls a function w more than 6 args, it’ll allocate space within the “argument build area” of its stack frame.</div><br><div><b>allocating and deallocating memory:</b>&nbsp;Q may need to allocate space for local variables when it begins and then free that storage before it returns. There are 3 common cases where this is needed:</div><ol><li><div>Not enough registers to hold all of the local data</div></li><li><div>The address operator &amp; is applied to a local variables, and hence we must be able to generate an address for it</div></li><li><div>Some of the local variables are arrays or structures and hence must be accessed by array or structure references</div></li></ol><div>A procedure typically allocates space on the stack frame by decrementing the stack pointer. This results in the “local variables” portion of the stack frame. (see figure 3.32 for a good example of this)</div><br><div><b>The stack frame structure consists of 3 parts</b> (from top to bottom):</div><ol><li><div>Saved registers (useful for preserving callee-saved registers)</div></li><li><div>Local variables</div></li><li><div>Argument build area</div></li></ol><br><div>A stack frame is not required when:<br><br>1. All of the local variables can be held in registers, and<br><br>2. the function does not call any other functions (aka a _leaf procedure_) </div>	chapter3 CSAPP
<div>Which register is a procedure’s return value stored in? Name both the general purpose register and the media register (3.4.3, 3.11.0)</div>	<div>%rax or one of the low-order portions of this general purpose register.<br><br>If it is a float or double, then we use the %xmm0 media register</div>	chapter3 CSAPP
<div>Why do we have callee-saved registers and caller-saved registers? What’s the difference between them? (3.7.5)</div>	"<div>Although only one procedure can be active at a given time, we must make sure that when one procedure (the caller) calls another (the callee), the callee does not overwrite some register value that the caller planned to use later.</div><br><div>To handle this. x86-64 adopts a uniform set of conventions for register usage that must be respected by all procedures, including those in program libs.</div><br><div><b>callee-saved:</b>&nbsp;registers %rbx, %rbp, and %r12-%r15.</div><div>When procedure P calls procedure Q, Q must preserve the values of these registers, ensuring they have the same values when Q returns to P as they did when Q was called.</div><div>There are 2 ways procedure Q can preserve the register:</div><ol><li><div>Not use it at all</div></li><li><div>Pushing the registers onto the ""saved registers"" portion of the stack frame (eg: ""pushq %r15; pushq %r14"" at beginning, altering it, then ""popq %r14; popq %r15"" at end of the procedure before returning) (see figure 3.34)</div></li></ol><br><div><b>caller-saved:</b> all other registers, except for the stack pointer %rsp.</div><div>These registers can be modified by any function without having to save it first.</div><div>eg: Procedure P having some local data in such a register and calling procedure Q; since Q is free to alter this register, it is incumbent upon P (the caller) to first save the data before it makes the call.</div>"	chapter3 CSAPP
<div>How does x86 handle recursive code? (3.7.6)</div>	<div>Recursive code is possible since each procedure call has its own private space on the stack, and the local variables of the multiple outstanding calls do not interfere with one another. Thus, calling a function recursively proceeds just like any other function call.</div><br><div>The stack and register-saving disciplines (saved values of the return location and callee-saved registers) suffice to make recursive functions operate correctly.</div>	chapter3 CSAPP
Handling array access in asm (3.8.2)&nbsp;<div><br></div><div>How do we fill in the following table, given array of shorts, S (stored in memory location Xs), and i, stored in registers %rdx and %rcx, respectively:<br> <pre>| expr     | type | value | asm |<br>|----------+------+-------+-----|<br>| S+1      |      |       |     |<br>| S[3]     |      |       |     |<br>| &amp;S[i]    |      |       |     |<br>| S[4*i+1] |      |       |     |<br>| S+i-5    |      |       |     |<br></pre>  You can reference the following table of expressions for calculating an array E (stored at memory location Xe) with integer index i, stored in registers %rdx and %rcd, respectively:  <pre>| expr         | type | value           | asm                         |<br>|--------------+------+-----------------+-----------------------------|<br>| E            | int* | Xe              | movl %rdx, %rax             |<br>| E[0]         | int  | M[Xe]           | movl (%rdx), %eax           |<br>| E[i]         | int  | M[Xe + 4i]      | movl (%rdx,%rcx,4), %eax    |<br>| &amp;E[2]        | int* | Xe + 8          | leaq 8(%rdx),%rax           |<br>| E + i - 1    | int* | Xe + 4i - 4     | leaq -4(%rdx,%rcx,4), %rax  |<br>| *(E + 1 - 3) | int  | M[Xe + 4i - 12] | movl -12(%rdx,%rcx,4), %rax |<br>| &amp;E[i] - E    | long | i               | movq %rcx, %rax             |<br></pre> </div>	<pre> | expr     | type   | value          | asm                        | |----------+--------+----------------+----------------------------| | S+1      | short* | Xs+2           | leaq 2(%rdx),%rax          | | S[3]     | short  | M[Xs + 6]      | movw 6(%rdx),%ax           | | &amp;S[i]    | short* | Xs + 2i        | leaq (%rdx,%rcx, 2),%rax   | | S[4*i+1] | short  | M[Xs + 8i + 2] | movw 2(%rdx,%rcx,8),%ax    | | S+i-5    | short* | Xs + 2i - 10   | leaq -10(%rdx,%rcx,2),%rax | </pre> 	chapter3 CSAPP
"<div>Why does C provide a ""-&gt;"" notation for dereferencing a struct's pointer and selecting a field within it? Eg: what is rp-&gt;x equivalent to? (3.9.1)</div>"	"TLDR:<div><br></div><div>(*rp).x is the same as rp-&gt;x&nbsp;</div><div><br></div><div>This combo of deref and field selection is so common that C provides an alternative notation using -&gt;.&nbsp;<br></div><div><br></div><div><br></div><div>So given a struct like:  <pre>struct rect {   unsigned long width;   unsigned long height; } </pre>  The expression (*rp).width dereferences the pointer and selects the width field of the resulting structure. Parentheses are required, because the compiler would interpret the expression *rp.width as *(rp.width), which is not valid. Using the ""-&gt;"" notation, rp-&gt;width is equivalent to the expression (*rp).width. </div>"	chapter3 CSAPP
<div>What are the 2 main use cases for unions in C? Examples of each? (3.9.2)</div>	1. Reducing space allocated by bypassing the C type system, for mutually exclusive types.<br><br>Eg: Using a single struct to represent both leaf and internal nodes of a binary tree<br><br><pre>typedef enum { N_LEAF, N_INTERNAL } nodetype_t;<br><br>struct node_t {<br>  nodetype_t type;<br>  union {<br>    struct {<br>      struct node_t *left;<br>      struct node_t *right;<br>    } internal;<br>    double data[2];<br>  } info<br>}<br></pre><br><br>This way we can use a single struct to represent both nodes without wasting extra bits storing fields that go unused.<br>In this case, the savings gain of using a union is small relative to the awkwardness of the resulting code. For data structures with more fields, the savings can be more compelling.<br><br><br><br>2. Accessing bit patterns of different data types<br><br>In this code, we store the argument in the union using one data type, and access it using another.<br><br><pre>unsigned long double2bits(double d) {<br>  union {<br>    double d;<br>    unsigned long u;<br>  } temp;<br>  temp.d = d;<br>  return temp.u;<br>};<br></pre><br><br>u and d will have the same bit representations, but different numeric values, except for the case when d is 0.0.<br>	chapter3 CSAPP
"<div>What is the Intel x86-64 alignment rule? What's an ""alignment requirement"", and why does it matter? (3.9.3)</div>"	"Intel's alignment rule is based on the principle that any primitive object of K bytes must have an address that is a multiple of K:  <pre>| K | types              | |---+--------------------| | 1 | char               | | 2 | short              | | 4 | int,float          | | 8 | long,double,char * | </pre>  This K value is the alignment requirement of the primitive. In addition, the compiler may need to add padding to the end of the structure so that each element in an array of structs will satisfy its alignment requirement.<div><br></div><div>&nbsp;NOTE: For most x86-64 instructions, keeping data aligned improves efficiency, but it does not affect program behavior. (eg some SSE instructions have a mandatory alignment requirement, while AVX does not)&nbsp;</div><div><br></div><div>&nbsp;Here's an example demonstrating how we can optimize for alignment on a C struct:  <pre>struct {   char *a;   short b;   double c;   char d;   float e;   char f;   long g;   int h; }  | field  | a | b |  c |  d |  e |  f |  g |  h | |--------+---+---+----+----+----+----+----+----| | size   | 8 | 2 |  8 |  1 |  4 |  1 |  8 |  4 | | offset | 0 | 8 | 16 | 24 | 28 | 32 | 40 | 48 |  </pre>  The structure is 56 bytes long. The end of the structure must be padded by 4 bytes to satisfy the 8 byte alignment requirement.&nbsp;</div><div><br></div><div>&nbsp;We can improve the efficiency of this struct. For example, when all data elements have a length equal to a power of 2, we can order them by descending size:  <pre>struct {   char *a;   double c;   long g;   float e;   int h;   short b;   char d;   char f; }  | field  | a | c |  g |  e |  h |  b |  d |  f | |--------+---+---+----+----+----+----+----+----| | size   | 8 | 8 |  8 |  4 |  4 |  2 |  1 |  1 | | offset | 0 | 8 | 16 | 24 | 28 | 32 | 34 | 35 | </pre>  This new structure is 40 bytes long. (we still need to add 4 bytes of padding to satisfy the 8 byte alignment requirement)&nbsp;</div><div><br></div><div>The compiler also places directives in the asm indicating the desired alignment for global data. Eg: the asm of a jump table may contain the following directive: "".align 8"" This ensures that the data following it (eg: the start of the jump table) will start with an address that is a multiple of 8. Since each table entry is 8 bytes long, all success elements will obey the 8-byte alignment restriction.</div>"	chapter3 CSAPP
<div>What are some key principles of pointers and their mapping into machine code? (up to 7 principles, wrt types, casting, and referencing) (3.10.1)</div>	1. Every pointer has an associated type<br> - indicates what kind of object the pointer points to<br> - Pointer types are not part of machine code; they are an abstraction provided by C to help programmers avoid addressing errors<br><br><br>2. Every pointer has a value<br> - This value is an address of some object of the designated type<br> - THe special NULL 90) value indicates that the pointer does not point anywhere<br><br>3. Pointers are created with the '&amp;' value<br> - can be applied to any lvalue (expression appearing on the left side of an assignment)<br> - This operator often uses the leaq instruction to compute the expression value of, since it's designed to compute the address of a memory reference<br><br>4. Pointers are dereferenced with the '*' operator<br> - dereferencing is implemented by a memory reference - either storing to or retrieving from the specified address<br><br>5. Arrays and pointers are closely related<br> - Array referencing (eg: a[3]) has the exact same effect as pointer arithmetic and dereferencing (eg: *(a+3*L)) where L is size of data type associated with p<br><br>6. Casting from one type of pointer to another changes its type but not its value<br> - One effect of casting is to change any scaling of pointer arithmetic (see above)<br><br>7. Pointers can also point to functions<br> - This provides a powerful capability for storing and passing references to code, to be invoked by some other part of the program<br> - the value of a function pointer is the address of the first instruction in the machine-code representation of the function<br><div><br></div><br><pre>// given a function defined by prototype:<br>int fun(int x, int *p);<br><br>// then we can use function pointer:<br>int (*fp)(int, int *);<br>fp = fun;<br>int y = 1;<br>int result = fp(3, &amp;y)<br></pre><br>	chapter3 CSAPP
What is the difference between a worm and a virus? (3.10.3)	"<div>A worm is a program that can run by itself and can propagate a fully working version of itself to other machines.</div><br><div>A virus is a piece of code that adds itself to other programs, including OS's. It cannot run independently.</div><br><div>In popular press, the term ""virus"" is used to refer to a variety of different strategies for spreading attacking code amons systems, and so you'll hear people saying ""virus"" for what more properly should be called a ""worm"".</div>"	chapter3 CSAPP
How can we avoid buffer overflow corruption when writing data to a char *? What are 3 ways to thwart buffer overflow attacks? (3.10.3)	"<div>When reading/writing from the source to the destination buffer, we can avoid buffer overflow corruption by including a count for the max number of bytes that can be transferred (eg: passing the length of the buffer as an additional arg the the function that does the read/write)</div><br><div>3 ways of thwarting buffer overflow attacks:</div><div><br></div><div>1. Leverage stack randomization to prevent an attacker from passing control to the malicious code&nbsp;</div><div><br></div><div>ASLR (address space layout randomization) is now a common practice. Note that it can still be hacked via brute force different addresses. A common trick is to include a long sequence of no-op instructions before the actual exploit code, increasing the odds that an attacker will guess an address somewhere in this sequence. This is also called a no-op sled, as the program ""slides"" through the sequence into the malicious code.</div><div><br></div><div>2. Leverage _canarying_ for stack corruption detection (prob 3.48)<br><br></div><div>Canarying uses _segmented addressing_ to store a value in a read-only address segment, so that an attacker cannot overwrite it. By storing the canary value on the stack (ideally right after the buffer), we can perform the buffer modifications, then xor the stack's canary value with the value we have in the special segment. A non-zero xor result indicates that the canary on the stack has been modified, and so the code will call an error routine.<br><br></div><div>3. Leverage _memory protection_ to limiting executable code regions.</div><div><br></div><div>Memory Protection is where each page has hardware-supported read/write/execute perms. Logically dividing the virtual memory space in this way prevents an attacker from inserting executable code into a system.</div><div><br></div><div>For example, an NX (for ""no-execute"") bit was introduced into the memory protection for 64-bit processors, allowing the stack to be marked as readable and writable, but not executable. Performing this check is done in hardware, with no penalty in efficiency.</div><div><br></div><div><br></div><div><br></div>"	chapter3 CSAPP
How does asm support stack frames of variable size? (3.10.5)	<div>*Frame Pointer (aka Base Pointer)* - served by register %rbp, allows setting the initial position of the stack. We then keep %rbp pointing to this position throughout the execution of the function, and can reference local variables at offsets relative to %rbp.</div><br><div>At the end of the function, the frame pointer is restored to its previous value, eg:</div><br><div>movq $rbp, $rsp ; set stack pointer to beginning of frame</div><div>popq $rbp ; restore saved %rbp and set stack ptr to end of caller's frame</div><br><div>Historically, most compilers used frame pointers when generating IA32 code, but recent versions of GCC have dropped this convention. Now it's only used in cases where the stack frame may be of variable size. Note that it's acceptable to mix code that uses frame pointers with code that does not, as long as all functions treat %rbp as a callee-saved register.</div><br>	chapter3 CSAPP
<div>How can we ensure that a pointer on the stack is byte-aligned? Eg: if a structure's byte alignment is 8, and %rsp is 18, how can we store it at the correct address?(3.10.5)</div>	<div>We'll need to ensure any variable-length data structures on the stack are byte-aligned (eg: a variable length array). To do this, we can use a combination of biasing and shifting, for example, to round the data structure's location up to the nearest multiple.</div><br><div>Eg when saving an array of pointers, p, where the %rax is the original location of p:</div><br><div>leaq 7(%rsp), %rax ; add biasing to ensure we round up</div><div>shrq $3, %rax ; int division by 8</div><div>leaq 0(,%rax,8), %r8 ; multiply by 8 to store p's rounded-up address into %r8</div><br><div>Eg: if %rsp is at 18, this will assign %r8 to be 24. If %rsp is at 16, %r8 will be 16.</div>	chapter3 CSAPP
<div>What is SIMD and how does it relate to floating point architecture? What are 3 types of SIMD extensions? (3.11)</div>	<div>Single Instruction, Multiple Data is a mode of operation that allows instructions to be performed on multiple values in parallel. These extensions have progressed from MMX (multi-media) to SSE (streaming SIMD) to AVX (advanced vector).</div><br><div>Starting with SSE2, in 2000, these instructions have included ones to operate on scalar floating point data. All processors capable of executing x64-64 code support SSE2 or higher. When operating on scalar data, these registers only hold floating-point data, and only the lower 32 bits (for float) or 64 bits (for double) are used.</div><br>	chapter3 CSAPP
<div>How many registers are in AVX floating point architecture, and how much do they store? (3.11)</div>	<div>AVX floating-point architecture allows data to be stored in 16 YMM registers, named %ymm0-%ymm15. Each YMM register is 256 bits (32 bytes) long.</div><br><div>For scalar data, the asm code refers to the registers by their SSE XMM register names %xmm0-%xmm15, where each XMM register is the low-order 128 bits (16 bytes) of the corresponding YMM register.</div>	chapter3 CSAPP